{
  LOG.info("Starting to fetch from {}",this.partitions);
  final String clientId="flink-kafka-consumer-legacy-" + broker.id();
  int reconnects=0;
  try {
    consumer=new SimpleConsumer(broker.host(),broker.port(),soTimeout,bufferSize,clientId);
    getMissingOffsetsFromKafka(partitions);
    int offsetOutOfRangeCount=0;
    while (running) {
      List<FetchPartition> newPartitions=newPartitionsQueue.pollBatch();
      if (newPartitions != null) {
        getMissingOffsetsFromKafka(newPartitions);
        for (        FetchPartition newPartition : newPartitions) {
          if (partitions.contains(newPartition)) {
            throw new IllegalStateException("Adding partition " + newPartition + " to subscribed partitions even though it is already subscribed");
          }
          partitions.add(newPartition);
        }
        LOG.info("Adding {} new partitions to consumer thread {}",newPartitions.size(),getName());
        if (LOG.isDebugEnabled()) {
          LOG.debug("Partitions list: {}",newPartitions);
        }
      }
      if (partitions.size() == 0) {
        if (newPartitionsQueue.close()) {
          LOG.info("Consumer thread {} does not have any partitions assigned anymore. Stopping thread.",getName());
          running=false;
          unassignedPartitions.add(MARKER);
          break;
        }
 else {
          continue;
        }
      }
      FetchRequestBuilder frb=new FetchRequestBuilder();
      frb.clientId(clientId);
      frb.maxWait(maxWait);
      frb.minBytes(minBytes);
      for (      FetchPartition fp : partitions) {
        frb.addFetch(fp.topic,fp.partition,fp.nextOffsetToRead,fetchSize);
      }
      kafka.api.FetchRequest fetchRequest=frb.build();
      LOG.debug("Issuing fetch request {}",fetchRequest);
      FetchResponse fetchResponse;
      try {
        fetchResponse=consumer.fetch(fetchRequest);
      }
 catch (      Throwable cce) {
        if (cce instanceof ClosedChannelException) {
          LOG.warn("Fetch failed because of ClosedChannelException.");
          LOG.debug("Full exception",cce);
          if (++reconnects >= reconnectLimit) {
            LOG.warn("Unable to reach broker after {} retries. Returning all current partitions",reconnectLimit);
            for (            FetchPartition fp : this.partitions) {
              unassignedPartitions.add(fp);
            }
            this.partitions.clear();
            continue;
          }
          try {
            consumer.close();
          }
 catch (          Throwable t) {
            LOG.warn("Error while closing consumer connection",t);
          }
          Thread.sleep(500);
          consumer=new SimpleConsumer(broker.host(),broker.port(),soTimeout,bufferSize,clientId);
          continue;
        }
 else {
          throw cce;
        }
      }
      reconnects=0;
      if (fetchResponse == null) {
        throw new RuntimeException("Fetch failed");
      }
      if (fetchResponse.hasError()) {
        String exception="";
        List<FetchPartition> partitionsToGetOffsetsFor=new ArrayList<>();
        Iterator<FetchPartition> partitionsIterator=partitions.iterator();
        boolean partitionsRemoved=false;
        while (partitionsIterator.hasNext()) {
          final FetchPartition fp=partitionsIterator.next();
          short code=fetchResponse.errorCode(fp.topic,fp.partition);
          if (code == ErrorMapping.OffsetOutOfRangeCode()) {
            partitionsToGetOffsetsFor.add(fp);
          }
 else           if (code == ErrorMapping.NotLeaderForPartitionCode() || code == ErrorMapping.LeaderNotAvailableCode() || code == ErrorMapping.BrokerNotAvailableCode() || code == ErrorMapping.UnknownCode()) {
            LOG.warn("{} is not the leader of {}. Reassigning leader for partition",broker,fp);
            LOG.debug("Error code = {}",code);
            unassignedPartitions.add(fp);
            partitionsIterator.remove();
            partitionsRemoved=true;
          }
 else           if (code != ErrorMapping.NoError()) {
            exception+="\nException for " + fp.topic + ":"+ fp.partition+ ": "+ StringUtils.stringifyException(ErrorMapping.exceptionFor(code));
          }
        }
        if (partitionsToGetOffsetsFor.size() > 0) {
          if (offsetOutOfRangeCount++ > 3) {
            throw new RuntimeException("Found invalid offsets more than three times in partitions " + partitionsToGetOffsetsFor.toString() + " "+ "Exceptions: "+ exception);
          }
          LOG.warn("The following partitions had an invalid offset: {}",partitionsToGetOffsetsFor);
          getLastOffset(consumer,partitionsToGetOffsetsFor,getInvalidOffsetBehavior(config));
          LOG.warn("The new partition offsets are {}",partitionsToGetOffsetsFor);
          continue;
        }
 else         if (partitionsRemoved) {
          continue;
        }
 else {
          throw new IOException("Error while fetching from broker '" + broker + "':"+ exception);
        }
      }
 else {
        offsetOutOfRangeCount=0;
      }
      int messagesInFetch=0;
      int deletedMessages=0;
      Iterator<FetchPartition> partitionsIterator=partitions.iterator();
      partitionsLoop:       while (partitionsIterator.hasNext()) {
        final FetchPartition fp=partitionsIterator.next();
        final ByteBufferMessageSet messageSet=fetchResponse.messageSet(fp.topic,fp.partition);
        final KafkaTopicPartition topicPartition=new KafkaTopicPartition(fp.topic,fp.partition);
        for (        MessageAndOffset msg : messageSet) {
          if (running) {
            messagesInFetch++;
            if (msg.offset() < fp.nextOffsetToRead) {
              LOG.info("Skipping message with offset " + msg.offset() + " because we have seen messages until "+ fp.nextOffsetToRead+ " from partition "+ fp.partition+ " already");
              continue;
            }
            final long offset=msg.offset();
            ByteBuffer payload=msg.message().payload();
            byte[] valueBytes;
            if (payload == null) {
              deletedMessages++;
              valueBytes=null;
            }
 else {
              valueBytes=new byte[payload.remaining()];
              payload.get(valueBytes);
            }
            byte[] keyBytes=null;
            int keySize=msg.message().keySize();
            if (keySize >= 0) {
              ByteBuffer keyPayload=msg.message().key();
              keyBytes=new byte[keySize];
              keyPayload.get(keyBytes);
            }
            final T value=deserializer.deserialize(keyBytes,valueBytes,fp.topic,fp.partition,offset);
            if (deserializer.isEndOfStream(value)) {
              partitionsIterator.remove();
              continue partitionsLoop;
            }
synchronized (sourceContext.getCheckpointLock()) {
              owner.flinkKafkaConsumer.processElement(sourceContext,topicPartition,value,offset);
            }
            fp.nextOffsetToRead=offset + 1;
          }
 else {
            return;
          }
        }
      }
      LOG.debug("This fetch contained {} messages ({} deleted messages)",messagesInFetch,deletedMessages);
    }
    if (!newPartitionsQueue.close()) {
      throw new Exception("Bug: Cleanly leaving fetcher thread without having a closed queue.");
    }
  }
 catch (  Throwable t) {
    owner.stopWithError(t);
  }
 finally {
    if (consumer != null) {
      try {
        consumer.close();
      }
 catch (      Throwable t) {
        LOG.error("Error while closing the Kafka simple consumer",t);
      }
    }
  }
}
