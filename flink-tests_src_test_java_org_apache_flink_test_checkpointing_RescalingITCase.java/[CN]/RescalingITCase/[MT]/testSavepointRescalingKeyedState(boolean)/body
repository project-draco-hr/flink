{
  final int numberKeys=42;
  final int numberElements=1000;
  final int numberElements2=500;
  final int parallelism=scaleOut ? numSlots / 2 : numSlots;
  final int parallelism2=scaleOut ? numSlots : numSlots / 2;
  final int maxParallelism=13;
  FiniteDuration timeout=new FiniteDuration(3,TimeUnit.MINUTES);
  Deadline deadline=timeout.fromNow();
  ActorGateway jobManager=null;
  JobID jobID=null;
  try {
    jobManager=cluster.getLeaderGateway(deadline.timeLeft());
    JobGraph jobGraph=createJobGraphWithKeyedState(parallelism,maxParallelism,numberKeys,numberElements,false,100);
    jobID=jobGraph.getJobID();
    cluster.submitJobDetached(jobGraph);
    SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(),TimeUnit.MILLISECONDS);
    Set<Tuple2<Integer,Integer>> actualResult=CollectionSink.getElementsSet();
    Set<Tuple2<Integer,Integer>> expectedResult=new HashSet<>();
    for (int key=0; key < numberKeys; key++) {
      int keyGroupIndex=KeyGroupRangeAssignment.assignToKeyGroup(key,maxParallelism);
      expectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism,parallelism,keyGroupIndex),numberElements * key));
    }
    assertEquals(expectedResult,actualResult);
    CollectionSink.clearElementsSet();
    Future<Object> savepointPathFuture=jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID),deadline.timeLeft());
    final String savepointPath=((JobManagerMessages.TriggerSavepointSuccess)Await.result(savepointPathFuture,deadline.timeLeft())).savepointPath();
    Future<Object> jobRemovedFuture=jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID),deadline.timeLeft());
    Future<Object> cancellationResponseFuture=jobManager.ask(new JobManagerMessages.CancelJob(jobID),deadline.timeLeft());
    Object cancellationResponse=Await.result(cancellationResponseFuture,deadline.timeLeft());
    assertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);
    Await.ready(jobRemovedFuture,deadline.timeLeft());
    jobID=null;
    JobGraph scaledJobGraph=createJobGraphWithKeyedState(parallelism2,maxParallelism,numberKeys,numberElements2,true,100);
    scaledJobGraph.setSavepointPath(savepointPath);
    jobID=scaledJobGraph.getJobID();
    cluster.submitJobAndWait(scaledJobGraph,false);
    jobID=null;
    Set<Tuple2<Integer,Integer>> actualResult2=CollectionSink.getElementsSet();
    Set<Tuple2<Integer,Integer>> expectedResult2=new HashSet<>();
    for (int key=0; key < numberKeys; key++) {
      int keyGroupIndex=KeyGroupRangeAssignment.assignToKeyGroup(key,maxParallelism);
      expectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism,parallelism2,keyGroupIndex),key * (numberElements + numberElements2)));
    }
    assertEquals(expectedResult2,actualResult2);
  }
  finally {
    CollectionSink.clearElementsSet();
    if (jobID != null && jobManager != null) {
      Future<Object> jobRemovedFuture=jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID),timeout);
      try {
        Await.ready(jobRemovedFuture,timeout);
      }
 catch (      TimeoutException|InterruptedException ie) {
        fail("Failed while cleaning up the cluster.");
      }
    }
  }
}
