{
  int numberKeys=42;
  int numberElements=1000;
  int numberElements2=500;
  int parallelism=numSlots / 2;
  int parallelism2=numSlots;
  int maxParallelism=13;
  FiniteDuration timeout=new FiniteDuration(3,TimeUnit.MINUTES);
  Deadline deadline=timeout.fromNow();
  ActorGateway jobManager=null;
  JobID jobID=null;
  try {
    jobManager=cluster.getLeaderGateway(deadline.timeLeft());
    JobGraph jobGraph=createPartitionedNonPartitionedStateJobGraph(parallelism,maxParallelism,parallelism,numberKeys,numberElements,false,100);
    jobID=jobGraph.getJobID();
    cluster.submitJobDetached(jobGraph);
    SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(),TimeUnit.MILLISECONDS);
    Set<Tuple2<Integer,Integer>> actualResult=CollectionSink.getElementsSet();
    Set<Tuple2<Integer,Integer>> expectedResult=new HashSet<>();
    HashKeyGroupAssigner<Integer> keyGroupAssigner=new HashKeyGroupAssigner<>(maxParallelism);
    for (int key=0; key < numberKeys; key++) {
      int keyGroupIndex=keyGroupAssigner.getKeyGroupIndex(key);
      expectedResult.add(Tuple2.of(keyGroupIndex % parallelism,numberElements * key));
    }
    assertEquals(expectedResult,actualResult);
    CollectionSink.clearElementsSet();
    Future<Object> savepointPathFuture=jobManager.ask(new JobManagerMessages.TriggerSavepoint(jobID),deadline.timeLeft());
    final String savepointPath=((JobManagerMessages.TriggerSavepointSuccess)Await.result(savepointPathFuture,deadline.timeLeft())).savepointPath();
    Future<Object> jobRemovedFuture=jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID),deadline.timeLeft());
    Future<Object> cancellationResponseFuture=jobManager.ask(new JobManagerMessages.CancelJob(jobID),deadline.timeLeft());
    Object cancellationResponse=Await.result(cancellationResponseFuture,deadline.timeLeft());
    assertTrue(cancellationResponse instanceof JobManagerMessages.CancellationSuccess);
    Await.ready(jobRemovedFuture,deadline.timeLeft());
    jobID=null;
    JobGraph scaledJobGraph=createPartitionedNonPartitionedStateJobGraph(parallelism2,maxParallelism,parallelism,numberKeys,numberElements + numberElements2,true,100);
    scaledJobGraph.setSavepointPath(savepointPath);
    jobID=scaledJobGraph.getJobID();
    cluster.submitJobAndWait(scaledJobGraph,false);
    jobID=null;
    Set<Tuple2<Integer,Integer>> actualResult2=CollectionSink.getElementsSet();
    Set<Tuple2<Integer,Integer>> expectedResult2=new HashSet<>();
    HashKeyGroupAssigner<Integer> keyGroupAssigner2=new HashKeyGroupAssigner<>(maxParallelism);
    for (int key=0; key < numberKeys; key++) {
      int keyGroupIndex=keyGroupAssigner2.getKeyGroupIndex(key);
      expectedResult2.add(Tuple2.of(keyGroupIndex % parallelism2,key * (numberElements + numberElements2)));
    }
    assertEquals(expectedResult2,actualResult2);
  }
  finally {
    CollectionSink.clearElementsSet();
    if (jobID != null && jobManager != null) {
      Future<Object> jobRemovedFuture=jobManager.ask(new TestingJobManagerMessages.NotifyWhenJobRemoved(jobID),timeout);
      try {
        Await.ready(jobRemovedFuture,timeout);
      }
 catch (      TimeoutException|InterruptedException ie) {
        fail("Failed while cleaning up the cluster.");
      }
    }
  }
}
