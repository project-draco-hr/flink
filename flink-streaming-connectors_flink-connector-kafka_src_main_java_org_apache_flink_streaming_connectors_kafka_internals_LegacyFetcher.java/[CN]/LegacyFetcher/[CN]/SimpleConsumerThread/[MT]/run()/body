{
  LOG.info("Starting to fetch from {}",Arrays.toString(this.partitions));
  try {
    final String clientId="flink-kafka-consumer-legacy-" + broker.id();
    final int soTimeout=Integer.valueOf(config.getProperty("socket.timeout.ms","30000"));
    final int bufferSize=Integer.valueOf(config.getProperty("socket.receive.buffer.bytes","65536"));
    final int fetchSize=Integer.valueOf(config.getProperty("fetch.message.max.bytes","1048576"));
    final int maxWait=Integer.valueOf(config.getProperty("fetch.wait.max.ms","100"));
    final int minBytes=Integer.valueOf(config.getProperty("fetch.min.bytes","1"));
    consumer=new SimpleConsumer(broker.host(),broker.port(),soTimeout,bufferSize,clientId);
{
      List<FetchPartition> partitionsToGetOffsetsFor=new ArrayList<>();
      for (      FetchPartition fp : partitions) {
        if (fp.nextOffsetToRead == FlinkKafkaConsumer.OFFSET_NOT_SET) {
          partitionsToGetOffsetsFor.add(fp);
        }
      }
      if (partitionsToGetOffsetsFor.size() > 0) {
        getLastOffset(consumer,partitionsToGetOffsetsFor,getInvalidOffsetBehavior(config));
        LOG.info("No prior offsets found for some partitions. Fetched the following start offsets {}",partitionsToGetOffsetsFor);
      }
    }
    int offsetOutOfRangeCount=0;
    while (running) {
      FetchRequestBuilder frb=new FetchRequestBuilder();
      frb.clientId(clientId);
      frb.maxWait(maxWait);
      frb.minBytes(minBytes);
      for (      FetchPartition fp : partitions) {
        frb.addFetch(fp.topic,fp.partition,fp.nextOffsetToRead,fetchSize);
      }
      kafka.api.FetchRequest fetchRequest=frb.build();
      LOG.debug("Issuing fetch request {}",fetchRequest);
      FetchResponse fetchResponse=consumer.fetch(fetchRequest);
      if (fetchResponse.hasError()) {
        String exception="";
        List<FetchPartition> partitionsToGetOffsetsFor=new ArrayList<>();
        for (        FetchPartition fp : partitions) {
          short code=fetchResponse.errorCode(fp.topic,fp.partition);
          if (code == ErrorMapping.OffsetOutOfRangeCode()) {
            partitionsToGetOffsetsFor.add(fp);
          }
 else           if (code != ErrorMapping.NoError()) {
            exception+="\nException for partition " + fp.partition + ": "+ StringUtils.stringifyException(ErrorMapping.exceptionFor(code));
          }
        }
        if (partitionsToGetOffsetsFor.size() > 0) {
          if (offsetOutOfRangeCount++ > 0) {
            throw new RuntimeException("Found invalid offsets more than once in partitions " + partitionsToGetOffsetsFor.toString() + " "+ "Exceptions: "+ exception);
          }
          LOG.warn("The following partitions had an invalid offset: {}",partitionsToGetOffsetsFor);
          getLastOffset(consumer,partitionsToGetOffsetsFor,getInvalidOffsetBehavior(config));
          LOG.warn("The new partition offsets are {}",partitionsToGetOffsetsFor);
          continue;
        }
 else {
          throw new IOException("Error while fetching from broker: " + exception);
        }
      }
      int messagesInFetch=0;
      int deletedMessages=0;
      for (      FetchPartition fp : partitions) {
        final ByteBufferMessageSet messageSet=fetchResponse.messageSet(fp.topic,fp.partition);
        final KafkaTopicPartition topicPartition=new KafkaTopicPartition(fp.topic,fp.partition);
        for (        MessageAndOffset msg : messageSet) {
          if (running) {
            messagesInFetch++;
            if (msg.offset() < fp.nextOffsetToRead) {
              LOG.info("Skipping message with offset " + msg.offset() + " because we have seen messages until "+ fp.nextOffsetToRead+ " from partition "+ fp.partition+ " already");
              continue;
            }
            final long offset=msg.offset();
            ByteBuffer payload=msg.message().payload();
            if (payload == null) {
              deletedMessages++;
synchronized (sourceContext.getCheckpointLock()) {
                offsetsState.put(topicPartition,offset);
              }
              continue;
            }
            byte[] valueBytes=new byte[payload.remaining()];
            payload.get(valueBytes);
            byte[] keyBytes=null;
            int keySize=msg.message().keySize();
            if (keySize >= 0) {
              ByteBuffer keyPayload=msg.message().key();
              keyBytes=new byte[keySize];
              keyPayload.get(keyBytes);
            }
            final T value=deserializer.deserialize(keyBytes,valueBytes,fp.topic,offset);
synchronized (sourceContext.getCheckpointLock()) {
              sourceContext.collect(value);
              offsetsState.put(topicPartition,offset);
            }
            fp.nextOffsetToRead=offset + 1;
          }
 else {
            return;
          }
        }
      }
      LOG.debug("This fetch contained {} messages ({} deleted messages)",messagesInFetch,deletedMessages);
    }
  }
 catch (  Throwable t) {
    owner.stopWithError(t);
  }
 finally {
    if (consumer != null) {
      try {
        consumer.close();
      }
 catch (      Throwable t) {
        LOG.error("Error while closing the Kafka simple consumer",t);
      }
    }
  }
}
