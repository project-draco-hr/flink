{
  try {
    Field offsetsField=FlinkKafkaConsumerBase.class.getDeclaredField("partitionState");
    Field runningField=FlinkKafkaConsumerBase.class.getDeclaredField("running");
    Field mapField=FlinkKafkaConsumerBase.class.getDeclaredField("pendingCheckpoints");
    offsetsField.setAccessible(true);
    runningField.setAccessible(true);
    mapField.setAccessible(true);
    FlinkKafkaConsumer08<?> consumer=mock(FlinkKafkaConsumer08.class);
    when(consumer.snapshotState(anyLong(),anyLong())).thenCallRealMethod();
    HashMap<KafkaTopicPartition,KafkaPartitionState> testState=new HashMap<>();
    HashMap<KafkaTopicPartition,Long> testOffsets=new HashMap<>();
    long[] offsets=new long[]{43,6146,133,16,162,616};
    int j=0;
    for (    long i : offsets) {
      KafkaTopicPartition ktp=new KafkaTopicPartition("topic",j++);
      testState.put(ktp,new KafkaPartitionState(ktp.getPartition(),i));
      testOffsets.put(ktp,i);
    }
    LinkedMap map=new LinkedMap();
    offsetsField.set(consumer,testState);
    runningField.set(consumer,true);
    mapField.set(consumer,map);
    assertTrue(map.isEmpty());
    for (long checkpointId=10L; checkpointId <= 2000L; checkpointId+=9L) {
      HashMap<KafkaTopicPartition,Long> checkpoint=consumer.snapshotState(checkpointId,47 * checkpointId);
      assertEquals(testOffsets,checkpoint);
      HashMap<KafkaTopicPartition,Long> checkpointCopy=(HashMap<KafkaTopicPartition,Long>)checkpoint.clone();
      for (      Map.Entry<KafkaTopicPartition,Long> e : testOffsets.entrySet()) {
        KafkaTopicPartition ktp=e.getKey();
        testState.put(ktp,new KafkaPartitionState(ktp.getPartition(),e.getValue() + 1));
        testOffsets.put(ktp,e.getValue() + 1);
      }
      assertEquals(checkpointCopy,checkpoint);
      assertTrue(map.size() > 0);
      assertTrue(map.size() <= FlinkKafkaConsumer08.MAX_NUM_PENDING_CHECKPOINTS);
    }
  }
 catch (  Exception e) {
    e.printStackTrace();
    fail(e.getMessage());
  }
}
