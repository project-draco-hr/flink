{
  String topic="brokerFailureTestTopic";
  createTestTopic(topic,2,2);
  LOG.info("Writing data to topic {}",topic);
  StreamExecutionEnvironment env=StreamExecutionEnvironment.createLocalEnvironment(1);
  DataStream<String> stream=env.addSource(new SourceFunction<String>(){
    private static final long serialVersionUID=1L;
    boolean running=true;
    @Override public void run(    SourceContext<String> ctx) throws Exception {
      LOG.info("Starting source.");
      int cnt=0;
      while (running) {
        String msg="kafka-" + cnt++;
        ctx.collect(msg);
        LOG.info("sending message = " + msg);
        if ((cnt - 1) % 20 == 0) {
          LOG.debug("Sending message #{}",cnt - 1);
        }
        if (cnt == 200) {
          LOG.info("Stopping to produce after 200 msgs");
          break;
        }
      }
    }
    @Override public void cancel(){
      LOG.info("Source got chancel()");
      running=false;
    }
  }
);
  stream.addSink(new KafkaSink<String>(brokerConnectionStrings,topic,new JavaDefaultStringSchema())).setParallelism(1);
  tryExecute(env,"broker failure test - writer");
  LOG.info("Reading data from topic {} and let a broker fail",topic);
  PartitionMetadata firstPart=null;
  do {
    if (firstPart != null) {
      LOG.info("Unable to find leader. error code {}",firstPart.errorCode());
      Thread.sleep(150);
    }
    Seq<PartitionMetadata> partitionMetadata=AdminUtils.fetchTopicMetadataFromZk(topic,zkClient).partitionsMetadata();
    firstPart=partitionMetadata.head();
  }
 while (firstPart.errorCode() != 0);
  final String leaderToShutDown=firstPart.leader().get().connectionString();
  LOG.info("Leader to shutdown {}",leaderToShutDown);
  final Thread brokerShutdown=new Thread(new Runnable(){
    @Override public void run(){
      shutdownKafkaBroker=false;
      while (!shutdownKafkaBroker) {
        try {
          Thread.sleep(10);
        }
 catch (        InterruptedException e) {
          LOG.warn("Interruption",e);
        }
      }
      for (      KafkaServer kafkaServer : brokers) {
        if (leaderToShutDown.equals(kafkaServer.config().advertisedHostName() + ":" + kafkaServer.config().advertisedPort())) {
          LOG.info("Killing Kafka Server {}",leaderToShutDown);
          kafkaServer.shutdown();
          leaderHasShutDown=true;
          break;
        }
      }
    }
  }
);
  brokerShutdown.start();
  DataStreamSource<String> consuming=env.addSource(new PersistentKafkaSource<String>(topic,new JavaDefaultStringSchema(),standardCC));
  consuming.setParallelism(1);
  consuming.addSink(new SinkFunction<String>(){
    private static final long serialVersionUID=1L;
    int elCnt=0;
    int start=0;
    int numOfMessagesToBeCorrect=100;
    int stopAfterMessages=150;
    BitSet validator=new BitSet(numOfMessagesToBeCorrect + 1);
    @Override public void invoke(    String value) throws Exception {
      LOG.info("Got message = " + value + " leader has shut down "+ leaderHasShutDown+ " el cnt = "+ elCnt+ " to rec"+ numOfMessagesToBeCorrect);
      String[] sp=value.split("-");
      int v=Integer.parseInt(sp[1]);
      if (start == -1) {
        start=v;
      }
      int offset=v - start;
      Assert.assertFalse("Received tuple with value " + offset + " twice",validator.get(offset));
      if (v - start < 0 && LOG.isWarnEnabled()) {
        LOG.warn("Not in order: {}",value);
      }
      validator.set(offset);
      elCnt++;
      if (elCnt == 20) {
        LOG.info("Asking leading broker to shut down");
        shutdownKafkaBroker=true;
      }
      if (shutdownKafkaBroker) {
        Thread.sleep(20);
      }
      if (leaderHasShutDown) {
        if (elCnt >= stopAfterMessages) {
          int nc;
          if ((nc=validator.nextClearBit(0)) < numOfMessagesToBeCorrect) {
            throw new RuntimeException("The bitset was not set to 1 on all elements to be checked. Next clear:" + nc + " Set: "+ validator);
          }
          throw new SuccessException();
        }
      }
    }
  }
);
  tryExecute(env,"broker failure test - reader");
}
