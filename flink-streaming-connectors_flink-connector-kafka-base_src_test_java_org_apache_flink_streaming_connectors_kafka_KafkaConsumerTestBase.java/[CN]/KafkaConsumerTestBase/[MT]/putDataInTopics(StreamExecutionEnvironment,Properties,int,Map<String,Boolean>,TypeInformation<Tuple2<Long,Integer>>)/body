{
  if (topics.size() != 2) {
    throw new RuntimeException("This method accepts two topics as arguments.");
  }
  TypeInformationSerializationSchema<Tuple2<Long,Integer>> sinkSchema=new TypeInformationSerializationSchema<>(outputTypeInfo,env.getConfig());
  DataStream<Tuple2<Long,Integer>> stream=env.addSource(new RichParallelSourceFunction<Tuple2<Long,Integer>>(){
    private boolean running=true;
    @Override public void run(    SourceContext<Tuple2<Long,Integer>> ctx) throws InterruptedException {
      int topic=0;
      int currentTs=1;
      while (running && currentTs < elementsPerPartition) {
        long timestamp=(currentTs % 10 == 0) ? -1L : currentTs;
        ctx.collect(new Tuple2<Long,Integer>(timestamp,topic));
        currentTs++;
      }
      Tuple2<Long,Integer> toWrite2=new Tuple2<Long,Integer>(-1L,topic);
      ctx.collect(toWrite2);
    }
    @Override public void cancel(){
      running=false;
    }
  }
).setParallelism(1);
  List<Map.Entry<String,Boolean>> topicsL=new ArrayList<>(topics.entrySet());
  stream.map(new MapFunction<Tuple2<Long,Integer>,Tuple2<Long,Integer>>(){
    @Override public Tuple2<Long,Integer> map(    Tuple2<Long,Integer> value) throws Exception {
      return value;
    }
  }
).setParallelism(1).addSink(kafkaServer.getProducer(topicsL.get(0).getKey(),new KeyedSerializationSchemaWrapper<>(sinkSchema),producerProperties,null)).setParallelism(1);
  if (!topicsL.get(1).getValue()) {
    stream.map(new MapFunction<Tuple2<Long,Integer>,Tuple2<Long,Integer>>(){
      @Override public Tuple2<Long,Integer> map(      Tuple2<Long,Integer> value) throws Exception {
        long timestamp=(value.f0 == -1) ? -1L : 1000 + value.f0;
        return new Tuple2<Long,Integer>(timestamp,1);
      }
    }
).setParallelism(1).addSink(kafkaServer.getProducer(topicsL.get(1).getKey(),new KeyedSerializationSchemaWrapper<>(sinkSchema),producerProperties,null)).setParallelism(1);
  }
}
