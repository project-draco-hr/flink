{
  final String topic="brokerFailureTestTopic";
  final int parallelism=2;
  final int numElementsPerPartition=1000;
  final int totalElements=parallelism * numElementsPerPartition;
  final int failAfterElements=numElementsPerPartition / 3;
  createTestTopic(topic,parallelism,2);
  DataGenerators.generateRandomizedIntegerSequence(StreamExecutionEnvironment.createRemoteEnvironment("localhost",flinkPort),kafkaServer,topic,parallelism,numElementsPerPartition,true);
  int leaderId=kafkaServer.getLeaderToShutDown(topic);
  LOG.info("Leader to shutdown {}",leaderId);
  DeserializationSchema<Integer> schema=new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO,new ExecutionConfig());
  StreamExecutionEnvironment env=StreamExecutionEnvironment.createRemoteEnvironment("localhost",flinkPort);
  env.setParallelism(parallelism);
  env.enableCheckpointing(500);
  env.setRestartStrategy(RestartStrategies.noRestart());
  env.getConfig().disableSysoutLogging();
  Properties props=new Properties();
  props.putAll(standardProps);
  props.putAll(secureProps);
  FlinkKafkaConsumerBase<Integer> kafkaSource=kafkaServer.getConsumer(topic,schema,props);
  env.addSource(kafkaSource).map(new PartitionValidatingMapper(parallelism,1)).map(new BrokerKillingMapper<Integer>(leaderId,failAfterElements)).addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1);
  BrokerKillingMapper.killedLeaderBefore=false;
  tryExecute(env,"Broker failure once test");
  kafkaServer.restartBroker(leaderId);
}
