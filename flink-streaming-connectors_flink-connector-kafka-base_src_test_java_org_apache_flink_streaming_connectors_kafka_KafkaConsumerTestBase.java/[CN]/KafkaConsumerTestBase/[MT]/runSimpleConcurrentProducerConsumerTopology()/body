{
  final String topic="concurrentProducerConsumerTopic_" + UUID.randomUUID().toString();
  final String additionalEmptyTopic="additionalEmptyTopic_" + UUID.randomUUID().toString();
  final int parallelism=3;
  final int elementsPerPartition=100;
  final int totalElements=parallelism * elementsPerPartition;
  createTestTopic(topic,parallelism,2);
  createTestTopic(additionalEmptyTopic,parallelism,1);
  final StreamExecutionEnvironment env=StreamExecutionEnvironment.createRemoteEnvironment("localhost",flinkPort);
  env.setParallelism(parallelism);
  env.enableCheckpointing(500);
  env.setRestartStrategy(RestartStrategies.noRestart());
  env.getConfig().disableSysoutLogging();
  TypeInformation<Tuple2<Long,String>> longStringType=TypeInfoParser.parse("Tuple2<Long, String>");
  TypeInformationSerializationSchema<Tuple2<Long,String>> sourceSchema=new TypeInformationSerializationSchema<>(longStringType,env.getConfig());
  TypeInformationSerializationSchema<Tuple2<Long,String>> sinkSchema=new TypeInformationSerializationSchema<>(longStringType,env.getConfig());
  DataStream<Tuple2<Long,String>> stream=env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>(){
    private boolean running=true;
    @Override public void run(    SourceContext<Tuple2<Long,String>> ctx) throws InterruptedException {
      int cnt=getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition;
      int limit=cnt + elementsPerPartition;
      while (running && cnt < limit) {
        ctx.collect(new Tuple2<>(1000L + cnt,"kafka-" + cnt));
        cnt++;
        Thread.sleep(50);
      }
    }
    @Override public void cancel(){
      running=false;
    }
  }
);
  Properties producerProperties=FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings);
  producerProperties.setProperty("retries","3");
  FlinkKafkaProducerBase<Tuple2<Long,String>> prod=kafkaServer.getProducer(topic,new KeyedSerializationSchemaWrapper<>(sinkSchema),producerProperties,null);
  stream.addSink(prod);
  List<String> topics=new ArrayList<>();
  topics.add(topic);
  topics.add(additionalEmptyTopic);
  FlinkKafkaConsumerBase<Tuple2<Long,String>> source=kafkaServer.getConsumer(topics,sourceSchema,standardProps);
  DataStreamSource<Tuple2<Long,String>> consuming=env.addSource(source).setParallelism(parallelism);
  consuming.addSink(new RichSinkFunction<Tuple2<Long,String>>(){
    private int elCnt=0;
    private BitSet validator=new BitSet(totalElements);
    @Override public void invoke(    Tuple2<Long,String> value) throws Exception {
      String[] sp=value.f1.split("-");
      int v=Integer.parseInt(sp[1]);
      assertEquals(value.f0 - 1000,(long)v);
      assertFalse("Received tuple twice",validator.get(v));
      validator.set(v);
      elCnt++;
      if (elCnt == totalElements) {
        int nc;
        if ((nc=validator.nextClearBit(0)) != totalElements) {
          fail("The bitset was not set to 1 on all elements. Next clear:" + nc + " Set: "+ validator);
        }
        throw new SuccessException();
      }
    }
    @Override public void close() throws Exception {
      super.close();
    }
  }
).setParallelism(1);
  try {
    tryExecutePropagateExceptions(env,"runSimpleConcurrentProducerConsumerTopology");
  }
 catch (  ProgramInvocationException|JobExecutionException e) {
    Throwable cause=e.getCause();
    int depth=0;
    while (cause != null && depth++ < 20) {
      if (cause instanceof kafka.common.NotLeaderForPartitionException) {
        throw (Exception)cause;
      }
      cause=cause.getCause();
    }
    throw e;
  }
  deleteTestTopic(topic);
}
