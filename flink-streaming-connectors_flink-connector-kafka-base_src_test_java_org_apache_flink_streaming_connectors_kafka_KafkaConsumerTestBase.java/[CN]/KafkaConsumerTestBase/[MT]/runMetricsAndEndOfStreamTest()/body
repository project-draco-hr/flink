{
  final String topic="testEndOfStream";
  createTestTopic(topic,1,1);
  final int ELEMENT_COUNT=300;
  StreamExecutionEnvironment env=StreamExecutionEnvironment.createRemoteEnvironment("localhost",flinkPort);
  env.setParallelism(1);
  env.getConfig().setRestartStrategy(RestartStrategies.noRestart());
  env.getConfig().disableSysoutLogging();
  writeSequence(env,topic,ELEMENT_COUNT,1);
  final StreamExecutionEnvironment env1=StreamExecutionEnvironment.createRemoteEnvironment("localhost",flinkPort);
  env1.setParallelism(1);
  env1.getConfig().setRestartStrategy(RestartStrategies.noRestart());
  env1.getConfig().disableSysoutLogging();
  DataStream<Tuple2<Integer,Integer>> fromKafka=env.addSource(kafkaServer.getConsumer(topic,new FixedNumberDeserializationSchema(ELEMENT_COUNT),standardProps));
  fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>,Void>(){
    @Override public void flatMap(    Tuple2<Integer,Integer> value,    Collector<Void> out) throws Exception {
    }
  }
);
  JobExecutionResult result=tryExecute(env,"Consume " + ELEMENT_COUNT + " elements from Kafka");
  Map<String,Object> accuResults=result.getAllAccumulatorResults();
  if (kafkaServer.getVersion().equals("0.9")) {
    Assert.assertTrue("Not enough accumulators from Kafka Consumer: " + accuResults.size(),accuResults.size() > 38);
  }
  deleteTestTopic(topic);
}
