{
  this.stormTopology=this.stormBuilder.createTopology();
  final FlinkTopology env=new FlinkTopology();
  env.setParallelism(1);
  final HashMap<String,HashMap<String,DataStream<Tuple>>> availableInputs=new HashMap<String,HashMap<String,DataStream<Tuple>>>();
  for (  final Entry<String,IRichSpout> spout : this.spouts.entrySet()) {
    final String spoutId=spout.getKey();
    final IRichSpout userSpout=spout.getValue();
    final FlinkOutputFieldsDeclarer declarer=new FlinkOutputFieldsDeclarer();
    userSpout.declareOutputFields(declarer);
    final HashMap<String,Fields> sourceStreams=declarer.outputStreams;
    this.outputStreams.put(spoutId,sourceStreams);
    declarers.put(spoutId,declarer);
    final HashMap<String,DataStream<Tuple>> outputStreams=new HashMap<String,DataStream<Tuple>>();
    final DataStreamSource<?> source;
    if (sourceStreams.size() == 1) {
      final SpoutWrapper<Tuple> spoutWrapperSingleOutput=new SpoutWrapper<Tuple>(userSpout,spoutId,null,null);
      spoutWrapperSingleOutput.setStormTopology(stormTopology);
      final String outputStreamId=(String)sourceStreams.keySet().toArray()[0];
      DataStreamSource<Tuple> src=env.addSource(spoutWrapperSingleOutput,spoutId,declarer.getOutputType(outputStreamId));
      outputStreams.put(outputStreamId,src);
      source=src;
    }
 else {
      final SpoutWrapper<SplitStreamType<Tuple>> spoutWrapperMultipleOutputs=new SpoutWrapper<SplitStreamType<Tuple>>(userSpout,spoutId,null,null);
      spoutWrapperMultipleOutputs.setStormTopology(stormTopology);
      @SuppressWarnings({"unchecked","rawtypes"}) DataStreamSource<SplitStreamType<Tuple>> multiSource=env.addSource(spoutWrapperMultipleOutputs,spoutId,(TypeInformation)TypeExtractor.getForClass(SplitStreamType.class));
      SplitStream<SplitStreamType<Tuple>> splitSource=multiSource.split(new StormStreamSelector<Tuple>());
      for (      String streamId : sourceStreams.keySet()) {
        SingleOutputStreamOperator<Tuple,?> outStream=splitSource.select(streamId).map(new SplitStreamMapper<Tuple>());
        outStream.getTransformation().setOutputType(declarer.getOutputType(streamId));
        outputStreams.put(streamId,outStream);
      }
      source=multiSource;
    }
    availableInputs.put(spoutId,outputStreams);
    int dop=1;
    final ComponentCommon common=stormTopology.get_spouts().get(spoutId).get_common();
    if (common.is_set_parallelism_hint()) {
      dop=common.get_parallelism_hint();
      source.setParallelism(dop);
    }
 else {
      common.set_parallelism_hint(1);
    }
    env.increaseNumberOfTasks(dop);
  }
  final HashMap<String,IRichBolt> unprocessedBolts=new HashMap<String,IRichBolt>();
  unprocessedBolts.putAll(this.bolts);
  final HashMap<String,Set<Entry<GlobalStreamId,Grouping>>> unprocessdInputsPerBolt=new HashMap<String,Set<Entry<GlobalStreamId,Grouping>>>();
  boolean makeProgress=true;
  while (unprocessedBolts.size() > 0) {
    if (!makeProgress) {
      throw new RuntimeException("Unable to build Topology. Could not connect the following bolts: " + unprocessedBolts.keySet());
    }
    makeProgress=false;
    final Iterator<Entry<String,IRichBolt>> boltsIterator=unprocessedBolts.entrySet().iterator();
    while (boltsIterator.hasNext()) {
      final Entry<String,IRichBolt> bolt=boltsIterator.next();
      final String boltId=bolt.getKey();
      final IRichBolt userBolt=bolt.getValue();
      final ComponentCommon common=stormTopology.get_bolts().get(boltId).get_common();
      Set<Entry<GlobalStreamId,Grouping>> unprocessedInputs=unprocessdInputsPerBolt.get(boltId);
      if (unprocessedInputs == null) {
        unprocessedInputs=new HashSet<Entry<GlobalStreamId,Grouping>>();
        unprocessedInputs.addAll(common.get_inputs().entrySet());
        unprocessdInputsPerBolt.put(boltId,unprocessedInputs);
      }
      final Iterator<Entry<GlobalStreamId,Grouping>> inputStreamsIterator=unprocessedInputs.iterator();
      while (inputStreamsIterator.hasNext()) {
        final Entry<GlobalStreamId,Grouping> stormInputStream=inputStreamsIterator.next();
        final String producerId=stormInputStream.getKey().get_componentId();
        final String inputStreamId=stormInputStream.getKey().get_streamId();
        final HashMap<String,DataStream<Tuple>> producer=availableInputs.get(producerId);
        if (producer != null) {
          makeProgress=true;
          DataStream<Tuple> inputStream=producer.get(inputStreamId);
          if (inputStream != null) {
            final FlinkOutputFieldsDeclarer declarer=new FlinkOutputFieldsDeclarer();
            userBolt.declareOutputFields(declarer);
            final HashMap<String,Fields> boltOutputStreams=declarer.outputStreams;
            this.outputStreams.put(boltId,boltOutputStreams);
            this.declarers.put(boltId,declarer);
            final Grouping grouping=stormInputStream.getValue();
            if (grouping.is_set_shuffle()) {
              inputStream=inputStream.rebalance();
            }
 else             if (grouping.is_set_fields()) {
              final List<String> fields=grouping.get_fields();
              if (fields.size() > 0) {
                FlinkOutputFieldsDeclarer prodDeclarer=this.declarers.get(producerId);
                inputStream=inputStream.keyBy(prodDeclarer.getGroupingFieldIndexes(inputStreamId,grouping.get_fields()));
              }
 else {
                inputStream=inputStream.global();
              }
            }
 else             if (grouping.is_set_all()) {
              inputStream=inputStream.broadcast();
            }
 else             if (!grouping.is_set_local_or_shuffle()) {
              throw new UnsupportedOperationException("Flink only supports (local-or-)shuffle, fields, all, and global grouping");
            }
            final SingleOutputStreamOperator<?,?> outputStream;
            if (boltOutputStreams.size() < 2) {
              String outputStreamId=null;
              if (boltOutputStreams.size() == 1) {
                outputStreamId=(String)boltOutputStreams.keySet().toArray()[0];
              }
              final TypeInformation<Tuple> outType=declarer.getOutputType(outputStreamId);
              final BoltWrapper<Tuple,Tuple> boltWrapperSingleOutput=new BoltWrapper<Tuple,Tuple>(userBolt,boltId,this.outputStreams.get(producerId).get(inputStreamId),null);
              boltWrapperSingleOutput.setStormTopology(stormTopology);
              final SingleOutputStreamOperator<Tuple,?> outStream=inputStream.transform(boltId,outType,boltWrapperSingleOutput);
              if (outType != null) {
                final HashMap<String,DataStream<Tuple>> op=new HashMap<String,DataStream<Tuple>>();
                op.put(outputStreamId,outStream);
                availableInputs.put(boltId,op);
              }
              outputStream=outStream;
            }
 else {
              final BoltWrapper<Tuple,SplitStreamType<Tuple>> boltWrapperMultipleOutputs=new BoltWrapper<Tuple,SplitStreamType<Tuple>>(userBolt,boltId,this.outputStreams.get(producerId).get(inputStreamId),null);
              boltWrapperMultipleOutputs.setStormTopology(stormTopology);
              @SuppressWarnings({"unchecked","rawtypes"}) final TypeInformation<SplitStreamType<Tuple>> outType=(TypeInformation)TypeExtractor.getForClass(SplitStreamType.class);
              final SingleOutputStreamOperator<SplitStreamType<Tuple>,?> multiStream=inputStream.transform(boltId,outType,boltWrapperMultipleOutputs);
              final SplitStream<SplitStreamType<Tuple>> splitStream=multiStream.split(new StormStreamSelector<Tuple>());
              final HashMap<String,DataStream<Tuple>> op=new HashMap<String,DataStream<Tuple>>();
              for (              String outputStreamId : boltOutputStreams.keySet()) {
                SingleOutputStreamOperator<Tuple,?> outStream=splitStream.select(outputStreamId).map(new SplitStreamMapper<Tuple>());
                outStream.getTransformation().setOutputType(declarer.getOutputType(outputStreamId));
                op.put(outputStreamId,outStream);
              }
              availableInputs.put(boltId,op);
              outputStream=multiStream;
            }
            int dop=1;
            if (common.is_set_parallelism_hint()) {
              dop=common.get_parallelism_hint();
              outputStream.setParallelism(dop);
            }
 else {
              common.set_parallelism_hint(1);
            }
            env.increaseNumberOfTasks(dop);
            inputStreamsIterator.remove();
          }
 else {
            throw new RuntimeException("Cannot connect '" + boltId + "' to '"+ producerId+ "'. Stream '"+ inputStreamId+ "' not found.");
          }
        }
      }
      if (unprocessedInputs.size() == 0) {
        boltsIterator.remove();
      }
    }
  }
  return env;
}
