{
  configuration.setInt("mapreduce.input.fileinputformat.split.minsize",minNumSplits);
  JobContext jobContext;
  try {
    jobContext=HadoopUtils.instantiateJobContext(configuration,new JobID());
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
  jobContext.getCredentials().addAll(this.credentials);
  Credentials currentUserCreds=getCredentialsFromUGI(UserGroupInformation.getCurrentUser());
  if (currentUserCreds != null) {
    jobContext.getCredentials().addAll(currentUserCreds);
  }
  List<org.apache.hadoop.mapreduce.InputSplit> splits;
  try {
    splits=this.mapreduceInputFormat.getSplits(jobContext);
  }
 catch (  InterruptedException e) {
    throw new IOException("Could not get Splits.",e);
  }
  HadoopInputSplit[] hadoopInputSplits=new HadoopInputSplit[splits.size()];
  for (int i=0; i < hadoopInputSplits.length; i++) {
    hadoopInputSplits[i]=new HadoopInputSplit(i,splits.get(i),jobContext);
  }
  return hadoopInputSplits;
}
