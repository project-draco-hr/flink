{
  final Set<String> uniqFilesFound=new HashSet<>();
  FileCreator fc=new FileCreator(INTERVAL,NO_OF_FILES);
  fc.start();
  Thread t=new Thread(new Runnable(){
    @Override public void run(){
      TextInputFormat format=new TextInputFormat(new Path(hdfsURI));
      format.setFilesFilter(FilePathFilter.createDefaultFilter());
      ContinuousFileMonitoringFunction<String> monitoringFunction=new ContinuousFileMonitoringFunction<>(format,hdfsURI,FileProcessingMode.PROCESS_CONTINUOUSLY,1,INTERVAL);
      try {
        monitoringFunction.open(new Configuration());
        monitoringFunction.run(new TestingSourceContext(monitoringFunction,uniqFilesFound));
      }
 catch (      Exception e) {
      }
    }
  }
);
  t.start();
synchronized (uniqFilesFound) {
    uniqFilesFound.wait();
  }
  t.interrupt();
  fc.join();
  Assert.assertTrue(fc.getFilesCreated().size() == NO_OF_FILES);
  Assert.assertTrue(uniqFilesFound.size() == NO_OF_FILES);
  Set<org.apache.hadoop.fs.Path> filesCreated=fc.getFilesCreated();
  Set<String> fileNamesCreated=new HashSet<>();
  for (  org.apache.hadoop.fs.Path path : fc.getFilesCreated()) {
    fileNamesCreated.add(path.toString());
  }
  for (  String file : uniqFilesFound) {
    Assert.assertTrue(fileNamesCreated.contains(file));
  }
  for (  org.apache.hadoop.fs.Path file : filesCreated) {
    hdfs.delete(file,false);
  }
}
