{
  try {
    ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
    env.setDegreeOfParallelism(DEFAULT_PARALLELISM);
    DataSet<Tuple2<Long,Long>> sourceA=env.generateSequence(1,10000000).map(new Duplicator<Long>());
    DataSet<Tuple2<Long,Long>> sourceB=env.generateSequence(1,10000000).map(new Duplicator<Long>());
    DataSet<Tuple2<Long,Long>> sourceC=env.generateSequence(1,10000000).map(new Duplicator<Long>());
    DataSet<Tuple2<Long,Long>> mapped=sourceA.coGroup(sourceB).where(0).equalTo(1).with(new CoGroupFunction<Tuple2<Long,Long>,Tuple2<Long,Long>,Tuple2<Long,Long>>(){
      @Override public void coGroup(      Iterable<Tuple2<Long,Long>> first,      Iterable<Tuple2<Long,Long>> second,      Collector<Tuple2<Long,Long>> out){
      }
    }
).map(new IdentityMapper<Tuple2<Long,Long>>());
    DataSet<Tuple2<Long,Long>> joined=sourceB.join(sourceC).where(0).equalTo(1).with(new DummyFlatJoinFunction<Tuple2<Long,Long>>());
    DataSet<Tuple2<Long,Long>> joined2=mapped.join(joined).where(1).equalTo(1).with(new DummyFlatJoinFunction<Tuple2<Long,Long>>());
    DataSet<Tuple2<Long,Long>> reduced=mapped.groupBy(1).reduceGroup(new Top1GroupReducer<Tuple2<Long,Long>>());
    reduced.cross(joined2).output(new DiscardingOutputFormat<Tuple2<Tuple2<Long,Long>,Tuple2<Long,Long>>>());
    joined2.output(new DiscardingOutputFormat<Tuple2<Long,Long>>());
    joined2.output(new DiscardingOutputFormat<Tuple2<Long,Long>>());
    Plan plan=env.createProgramPlan();
    OptimizedPlan oPlan=compileNoStats(plan);
    new JobGraphGenerator().compileJobGraph(oPlan);
  }
 catch (  Exception e) {
    e.printStackTrace();
    fail(e.getMessage());
  }
}
