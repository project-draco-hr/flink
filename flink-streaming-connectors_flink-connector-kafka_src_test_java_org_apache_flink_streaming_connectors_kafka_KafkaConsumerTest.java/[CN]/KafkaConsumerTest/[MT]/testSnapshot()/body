{
  try {
    Field offsetsField=FlinkKafkaConsumer.class.getDeclaredField("lastOffsets");
    Field runningField=FlinkKafkaConsumer.class.getDeclaredField("running");
    Field mapField=FlinkKafkaConsumer.class.getDeclaredField("pendingCheckpoints");
    offsetsField.setAccessible(true);
    runningField.setAccessible(true);
    mapField.setAccessible(true);
    FlinkKafkaConsumer<?> consumer=mock(FlinkKafkaConsumer.class);
    when(consumer.snapshotState(anyLong(),anyLong())).thenCallRealMethod();
    HashMap<KafkaTopicPartition,Long> testOffsets=new HashMap<>();
    long[] offsets=new long[]{43,6146,133,16,162,616};
    int j=0;
    for (    long i : offsets) {
      KafkaTopicPartition ktp=new KafkaTopicPartition("topic",j++);
      testOffsets.put(ktp,i);
    }
    LinkedMap map=new LinkedMap();
    offsetsField.set(consumer,testOffsets);
    runningField.set(consumer,true);
    mapField.set(consumer,map);
    assertTrue(map.isEmpty());
    for (long checkpointId=10L; checkpointId <= 2000L; checkpointId+=9L) {
      HashMap<KafkaTopicPartition,Long> checkpoint=consumer.snapshotState(checkpointId,47 * checkpointId);
      assertEquals(testOffsets,checkpoint);
      HashMap<KafkaTopicPartition,Long> checkpointCopy=(HashMap<KafkaTopicPartition,Long>)checkpoint.clone();
      for (      Map.Entry<KafkaTopicPartition,Long> e : testOffsets.entrySet()) {
        testOffsets.put(e.getKey(),e.getValue() + 1);
      }
      assertEquals(checkpointCopy,checkpoint);
      assertTrue(map.size() > 0);
      assertTrue(map.size() <= FlinkKafkaConsumer.MAX_NUM_PENDING_CHECKPOINTS);
    }
  }
 catch (  Exception e) {
    e.printStackTrace();
    fail(e.getMessage());
  }
}
