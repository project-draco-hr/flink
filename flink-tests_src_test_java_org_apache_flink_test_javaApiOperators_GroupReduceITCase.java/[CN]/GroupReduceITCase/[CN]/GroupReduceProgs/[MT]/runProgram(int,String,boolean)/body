{
switch (progId) {
case 1:
{
      final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
      DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
      DataSet<Tuple2<Integer,Long>> reduceDs=ds.groupBy(1).reduceGroup(new Tuple3GroupReduce());
      reduceDs.writeAsCsv(resultPath);
      env.execute();
      return "1,1\n" + "5,2\n" + "15,3\n"+ "34,4\n"+ "65,5\n"+ "111,6\n";
    }
case 2:
{
    final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
    DataSet<Tuple5<Integer,Long,Integer,String,Long>> ds=CollectionDataSets.get5TupleDataSet(env);
    DataSet<Tuple5<Integer,Long,Integer,String,Long>> reduceDs=ds.groupBy(4,0).reduceGroup(new Tuple5GroupReduce());
    reduceDs.writeAsCsv(resultPath);
    env.execute();
    return "1,1,0,P-),1\n" + "2,3,0,P-),1\n" + "2,2,0,P-),2\n"+ "3,9,0,P-),2\n"+ "3,6,0,P-),3\n"+ "4,17,0,P-),1\n"+ "4,17,0,P-),2\n"+ "5,11,0,P-),1\n"+ "5,29,0,P-),2\n"+ "5,25,0,P-),3\n";
  }
case 3:
{
  final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
  env.setDegreeOfParallelism(1);
  DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
  DataSet<Tuple3<Integer,Long,String>> reduceDs=ds.groupBy(1).sortGroup(2,Order.ASCENDING).reduceGroup(new Tuple3SortedGroupReduce());
  reduceDs.writeAsCsv(resultPath);
  env.execute();
  return "1,1,Hi\n" + "5,2,Hello-Hello world\n" + "15,3,Hello world, how are you?-I am fine.-Luke Skywalker\n"+ "34,4,Comment#1-Comment#2-Comment#3-Comment#4\n"+ "65,5,Comment#5-Comment#6-Comment#7-Comment#8-Comment#9\n"+ "111,6,Comment#10-Comment#11-Comment#12-Comment#13-Comment#14-Comment#15\n";
}
case 4:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
DataSet<Tuple2<Integer,Long>> reduceDs=ds.groupBy(new KeySelector<Tuple3<Integer,Long,String>,Long>(){
  private static final long serialVersionUID=1L;
  @Override public Long getKey(  Tuple3<Integer,Long,String> in){
    return in.f1;
  }
}
).reduceGroup(new Tuple3GroupReduce());
reduceDs.writeAsCsv(resultPath);
env.execute();
return "1,1\n" + "5,2\n" + "15,3\n"+ "34,4\n"+ "65,5\n"+ "111,6\n";
}
case 5:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<CustomType> ds=CollectionDataSets.getCustomTypeDataSet(env);
DataSet<CustomType> reduceDs=ds.groupBy(new KeySelector<CustomType,Integer>(){
private static final long serialVersionUID=1L;
@Override public Integer getKey(CustomType in){
  return in.myInt;
}
}
).reduceGroup(new CustomTypeGroupReduce());
reduceDs.writeAsText(resultPath);
env.execute();
return "1,0,Hello!\n" + "2,3,Hello!\n" + "3,12,Hello!\n"+ "4,30,Hello!\n"+ "5,60,Hello!\n"+ "6,105,Hello!\n";
}
case 6:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
DataSet<Tuple3<Integer,Long,String>> reduceDs=ds.reduceGroup(new AllAddingTuple3GroupReduce());
reduceDs.writeAsCsv(resultPath);
env.execute();
return "231,91,Hello World\n";
}
case 7:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<CustomType> ds=CollectionDataSets.getCustomTypeDataSet(env);
DataSet<CustomType> reduceDs=ds.reduceGroup(new AllAddingCustomTypeGroupReduce());
reduceDs.writeAsText(resultPath);
env.execute();
return "91,210,Hello!";
}
case 8:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<Integer> intDs=CollectionDataSets.getIntegerDataSet(env);
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
DataSet<Tuple3<Integer,Long,String>> reduceDs=ds.groupBy(1).reduceGroup(new BCTuple3GroupReduce()).withBroadcastSet(intDs,"ints");
reduceDs.writeAsCsv(resultPath);
env.execute();
return "1,1,55\n" + "5,2,55\n" + "15,3,55\n"+ "34,4,55\n"+ "65,5,55\n"+ "111,6,55\n";
}
case 9:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
DataSet<Tuple3<Integer,Long,String>> reduceDs=ds.groupBy(1).reduceGroup(new InputReturningTuple3GroupReduce());
reduceDs.writeAsCsv(resultPath);
env.execute();
return "11,1,Hi!\n" + "21,1,Hi again!\n" + "12,2,Hi!\n"+ "22,2,Hi again!\n"+ "13,2,Hi!\n"+ "23,2,Hi again!\n";
}
case 10:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<CustomType> ds=CollectionDataSets.getCustomTypeDataSet(env);
DataSet<CustomType> reduceDs=ds.groupBy(new KeySelector<CustomType,Integer>(){
private static final long serialVersionUID=1L;
@Override public Integer getKey(CustomType in){
return in.myInt;
}
}
).reduceGroup(new CustomTypeGroupReduceWithCombine());
reduceDs.writeAsText(resultPath);
env.execute();
if (collectionExecution) {
return null;
}
 else {
return "1,0,test1\n" + "2,3,test2\n" + "3,12,test3\n"+ "4,30,test4\n"+ "5,60,test5\n"+ "6,105,test6\n";
}
}
case 11:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(2);
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
DataSet<Tuple2<Integer,String>> reduceDs=ds.groupBy(1).reduceGroup(new Tuple3GroupReduceWithCombine());
reduceDs.writeAsCsv(resultPath);
env.execute();
if (collectionExecution) {
return null;
}
 else {
return "1,test1\n" + "5,test2\n" + "15,test3\n"+ "34,test4\n"+ "65,test5\n"+ "111,test6\n";
}
}
case 12:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env).map(new IdentityMapper<Tuple3<Integer,Long,String>>()).setParallelism(4);
Configuration cfg=new Configuration();
cfg.setString(PactCompiler.HINT_SHIP_STRATEGY,PactCompiler.HINT_SHIP_STRATEGY_REPARTITION);
DataSet<Tuple2<Integer,String>> reduceDs=ds.reduceGroup(new Tuple3AllGroupReduceWithCombine()).withParameters(cfg);
reduceDs.writeAsCsv(resultPath);
env.execute();
if (collectionExecution) {
return null;
}
 else {
return "322,testtesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttest\n";
}
}
case 13:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
DataSet<Tuple3<Integer,Long,String>> reduceDs=ds.groupBy(1).sortGroup(2,Order.DESCENDING).reduceGroup(new Tuple3SortedGroupReduce());
reduceDs.writeAsCsv(resultPath);
env.execute();
return "1,1,Hi\n" + "5,2,Hello world-Hello\n" + "15,3,Luke Skywalker-I am fine.-Hello world, how are you?\n"+ "34,4,Comment#4-Comment#3-Comment#2-Comment#1\n"+ "65,5,Comment#9-Comment#8-Comment#7-Comment#6-Comment#5\n"+ "111,6,Comment#15-Comment#14-Comment#13-Comment#12-Comment#11-Comment#10\n";
}
case 14:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<Tuple5<Integer,Long,Integer,String,Long>> ds=CollectionDataSets.get5TupleDataSet(env);
DataSet<Tuple5<Integer,Long,Integer,String,Long>> reduceDs=ds.groupBy(new KeySelector<Tuple5<Integer,Long,Integer,String,Long>,Tuple2<Integer,Long>>(){
private static final long serialVersionUID=1L;
@Override public Tuple2<Integer,Long> getKey(Tuple5<Integer,Long,Integer,String,Long> t){
return new Tuple2<Integer,Long>(t.f0,t.f4);
}
}
).reduceGroup(new Tuple5GroupReduce());
reduceDs.writeAsCsv(resultPath);
env.execute();
return "1,1,0,P-),1\n" + "2,3,0,P-),1\n" + "2,2,0,P-),2\n"+ "3,9,0,P-),2\n"+ "3,6,0,P-),3\n"+ "4,17,0,P-),1\n"+ "4,17,0,P-),2\n"+ "5,11,0,P-),1\n"+ "5,29,0,P-),2\n"+ "5,25,0,P-),3\n";
}
case 15:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
DataSet<Tuple3<Integer,Long,String>> reduceDs=ds.groupBy(1).sortGroup(0,Order.ASCENDING).reduceGroup(new OrderCheckingCombinableReduce());
reduceDs.writeAsCsv(resultPath);
env.execute();
return "1,1,Hi\n" + "2,2,Hello\n" + "4,3,Hello world, how are you?\n"+ "7,4,Comment#1\n"+ "11,5,Comment#5\n"+ "16,6,Comment#10\n";
}
case 16:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<CrazyNested> ds=CollectionDataSets.getCrazyNestedDataSet(env);
DataSet<Tuple2<String,Integer>> reduceDs=ds.groupBy("nest_Lvl1.nest_Lvl2.nest_Lvl3.nest_Lvl4.f1nal").reduceGroup(new GroupReduceFunction<CollectionDataSets.CrazyNested,Tuple2<String,Integer>>(){
private static final long serialVersionUID=1L;
@Override public void reduce(Iterable<CrazyNested> values,Collector<Tuple2<String,Integer>> out) throws Exception {
int c=0;
String n=null;
for (CrazyNested v : values) {
c++;
n=v.nest_Lvl1.nest_Lvl2.nest_Lvl3.nest_Lvl4.f1nal;
}
out.collect(new Tuple2<String,Integer>(n,c));
}
}
);
reduceDs.writeAsCsv(resultPath);
env.execute();
return "aa,1\nbb,2\ncc,3\n";
}
case 17:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<FromTupleWithCTor> ds=CollectionDataSets.getPojoExtendingFromTuple(env);
DataSet<Integer> reduceDs=ds.groupBy("special","f2").reduceGroup(new GroupReduceFunction<FromTupleWithCTor,Integer>(){
private static final long serialVersionUID=1L;
@Override public void reduce(Iterable<FromTupleWithCTor> values,Collector<Integer> out) throws Exception {
int c=0;
for (FromTuple v : values) {
c++;
}
out.collect(c);
}
}
);
reduceDs.writeAsText(resultPath);
env.execute();
return "3\n2\n";
}
case 18:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<PojoContainingTupleAndWritable> ds=CollectionDataSets.getPojoContainingTupleAndWritable(env);
DataSet<Integer> reduceDs=ds.groupBy("hadoopFan","theTuple.*").reduceGroup(new GroupReduceFunction<PojoContainingTupleAndWritable,Integer>(){
private static final long serialVersionUID=1L;
@Override public void reduce(Iterable<PojoContainingTupleAndWritable> values,Collector<Integer> out) throws Exception {
int c=0;
for (PojoContainingTupleAndWritable v : values) {
c++;
}
out.collect(c);
}
}
);
reduceDs.writeAsText(resultPath);
env.execute();
return "1\n5\n";
}
case 19:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
DataSet<Tuple3<Integer,CrazyNested,POJO>> ds=CollectionDataSets.getTupleContainingPojos(env);
DataSet<Integer> reduceDs=ds.groupBy("f0","f1.*").reduceGroup(new GroupReduceFunction<Tuple3<Integer,CrazyNested,POJO>,Integer>(){
private static final long serialVersionUID=1L;
@Override public void reduce(Iterable<Tuple3<Integer,CrazyNested,POJO>> values,Collector<Integer> out) throws Exception {
int c=0;
for (Tuple3<Integer,CrazyNested,POJO> v : values) {
c++;
}
out.collect(c);
}
}
);
reduceDs.writeAsText(resultPath);
env.execute();
return "3\n1\n";
}
case 20:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<Tuple3<Integer,Long,String>> ds=CollectionDataSets.get3TupleDataSet(env);
DataSet<Tuple3<Integer,Long,String>> reduceDs=ds.groupBy(1).sortGroup("f2",Order.DESCENDING).reduceGroup(new Tuple3SortedGroupReduce());
reduceDs.writeAsCsv(resultPath);
env.execute();
return "1,1,Hi\n" + "5,2,Hello world-Hello\n" + "15,3,Luke Skywalker-I am fine.-Hello world, how are you?\n"+ "34,4,Comment#4-Comment#3-Comment#2-Comment#1\n"+ "65,5,Comment#9-Comment#8-Comment#7-Comment#6-Comment#5\n"+ "111,6,Comment#15-Comment#14-Comment#13-Comment#12-Comment#11-Comment#10\n";
}
case 21:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<Tuple2<Tuple2<Integer,Integer>,String>> ds=CollectionDataSets.getGroupSortedNestedTupleDataSet(env);
DataSet<String> reduceDs=ds.groupBy("f1").sortGroup(0,Order.DESCENDING).reduceGroup(new NestedTupleReducer());
reduceDs.writeAsText(resultPath);
env.execute();
return "a--(2,1)-(1,3)-(1,2)-\n" + "b--(2,2)-\n" + "c--(4,9)-(3,6)-(3,3)-\n";
}
case 22:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<Tuple2<Tuple2<Integer,Integer>,String>> ds=CollectionDataSets.getGroupSortedNestedTupleDataSet(env);
DataSet<String> reduceDs=ds.groupBy("f1").sortGroup("f0.f0",Order.ASCENDING).sortGroup("f0.f1",Order.ASCENDING).reduceGroup(new NestedTupleReducer());
reduceDs.writeAsText(resultPath);
env.execute();
return "a--(1,2)-(1,3)-(2,1)-\n" + "b--(2,2)-\n" + "c--(3,3)-(3,6)-(4,9)-\n";
}
case 23:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<Tuple2<Tuple2<Integer,Integer>,String>> ds=CollectionDataSets.getGroupSortedNestedTupleDataSet(env);
DataSet<String> reduceDs=ds.groupBy("f1").sortGroup("f0.f0",Order.DESCENDING).reduceGroup(new NestedTupleReducer());
reduceDs.writeAsText(resultPath);
env.execute();
return "a--(2,1)-(1,3)-(1,2)-\n" + "b--(2,2)-\n" + "c--(4,9)-(3,3)-(3,6)-\n";
}
case 24:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<Tuple2<Tuple2<Integer,Integer>,String>> ds=CollectionDataSets.getGroupSortedNestedTupleDataSet(env);
DataSet<String> reduceDs=ds.groupBy("f1").sortGroup("f0.f0",Order.DESCENDING).sortGroup("f0.f1",Order.DESCENDING).reduceGroup(new NestedTupleReducer());
reduceDs.writeAsText(resultPath);
env.execute();
return "a--(2,1)-(1,3)-(1,2)-\n" + "b--(2,2)-\n" + "c--(4,9)-(3,6)-(3,3)-\n";
}
case 25:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<PojoContainingTupleAndWritable> ds=CollectionDataSets.getGroupSortedPojoContainingTupleAndWritable(env);
DataSet<String> reduceDs=ds.groupBy("hadoopFan").sortGroup("theTuple.f0",Order.DESCENDING).sortGroup("theTuple.f1",Order.DESCENDING).reduceGroup(new GroupReduceFunction<CollectionDataSets.PojoContainingTupleAndWritable,String>(){
@Override public void reduce(Iterable<PojoContainingTupleAndWritable> values,Collector<String> out) throws Exception {
boolean once=false;
StringBuilder concat=new StringBuilder();
for (PojoContainingTupleAndWritable value : values) {
if (!once) {
concat.append(value.hadoopFan.get());
concat.append("---");
once=true;
}
concat.append(value.theTuple);
concat.append("-");
}
out.collect(concat.toString());
}
}
);
reduceDs.writeAsText(resultPath);
env.execute();
return "1---(10,100)-\n" + "2---(30,600)-(30,400)-(30,200)-(20,201)-(20,200)-\n";
}
case 26:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<CollectionDataSets.PojoWithMultiplePojos> ds=CollectionDataSets.getPojoWithMultiplePojos(env);
DataSet<String> reduceDs=ds.groupBy("p2.a2").reduceGroup(new GroupReduceFunction<CollectionDataSets.PojoWithMultiplePojos,String>(){
@Override public void reduce(Iterable<CollectionDataSets.PojoWithMultiplePojos> values,Collector<String> out) throws Exception {
StringBuilder concat=new StringBuilder();
for (CollectionDataSets.PojoWithMultiplePojos value : values) {
concat.append(value.p2.a2);
}
out.collect(concat.toString());
}
}
);
reduceDs.writeAsText(resultPath);
env.execute();
return "b\nccc\nee\n";
}
case 27:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<CollectionDataSets.PojoWithCollection> ds=CollectionDataSets.getPojoWithCollection(env);
DataSet<String> reduceDs=ds.groupBy("key").reduceGroup(new GroupReduceFunction<CollectionDataSets.PojoWithCollection,String>(){
@Override public void reduce(Iterable<CollectionDataSets.PojoWithCollection> values,Collector<String> out) throws Exception {
StringBuilder concat=new StringBuilder();
concat.append("call");
for (CollectionDataSets.PojoWithCollection value : values) {
concat.append("For key " + value.key + " we got: ");
for (CollectionDataSets.Pojo1 p : value.pojos) {
concat.append("pojo.a=" + p.a);
}
}
out.collect(concat.toString());
}
}
);
reduceDs.writeAsText(resultPath);
env.execute();
return "callFor key 0 we got: pojo.a=apojo.a=bFor key 0 we got: pojo.a=a2pojo.a=b2\n";
}
case 28:
{
final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
env.setDegreeOfParallelism(1);
DataSet<CollectionDataSets.PojoWithCollection> ds=CollectionDataSets.getPojoWithCollection(env);
DataSet<String> reduceDs=ds.groupBy("bigInt").reduceGroup(new GroupReduceFunction<CollectionDataSets.PojoWithCollection,String>(){
@Override public void reduce(Iterable<CollectionDataSets.PojoWithCollection> values,Collector<String> out) throws Exception {
StringBuilder concat=new StringBuilder();
concat.append("call");
for (CollectionDataSets.PojoWithCollection value : values) {
concat.append("\nFor key " + value.bigInt + " we got:\n"+ value);
}
out.collect(concat.toString());
}
}
);
reduceDs.writeAsText(resultPath);
env.execute();
return "call\n" + "For key 92233720368547758070 we got:\n" + "PojoWithCollection{pojos.size()=2, key=0, sqlDate=2033-05-18, bigInt=92233720368547758070, bigDecimalKeepItNull=null, scalaBigInt=10, mixed=[{someKey=1}, /this/is/wrong, uhlala]}\n"+ "For key 92233720368547758070 we got:\n"+ "PojoWithCollection{pojos.size()=2, key=0, sqlDate=1976-05-03, bigInt=92233720368547758070, bigDecimalKeepItNull=null, scalaBigInt=31104000, mixed=null}\n";
}
default :
{
throw new IllegalArgumentException("Invalid program id");
}
}
}
