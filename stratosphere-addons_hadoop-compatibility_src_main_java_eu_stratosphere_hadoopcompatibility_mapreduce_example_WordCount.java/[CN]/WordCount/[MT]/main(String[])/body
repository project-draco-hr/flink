{
  if (args.length < 2) {
    System.err.println("Usage: WordCount <input path> <result path>");
    return;
  }
  final String inputPath=args[0];
  final String outputPath=args[1];
  final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
  env.setDegreeOfParallelism(1);
  Job job=Job.getInstance();
  HadoopInputFormat<LongWritable,Text> hadoopInputFormat=new HadoopInputFormat<LongWritable,Text>(new TextInputFormat(),LongWritable.class,Text.class,job);
  TextInputFormat.addInputPath(job,new Path(inputPath));
  DataSet<Tuple2<LongWritable,Text>> text=env.createInput(hadoopInputFormat);
  DataSet<Tuple2<String,Integer>> words=text.flatMap(new Tokenizer());
  DataSet<Tuple2<String,Integer>> result=words.groupBy(0).aggregate(Aggregations.SUM,1);
  DataSet<Tuple2<Text,IntWritable>> hadoopResult=result.map(new HadoopDatatypeMapper());
  HadoopOutputFormat<Text,IntWritable> hadoopOutputFormat=new HadoopOutputFormat<Text,IntWritable>(new TextOutputFormat<Text,IntWritable>(),job);
  hadoopOutputFormat.getConfiguration().set("mapreduce.output.textoutputformat.separator"," ");
  TextOutputFormat.setOutputPath(job,new Path(outputPath));
  hadoopResult.output(hadoopOutputFormat);
  env.execute("Word Count");
}
