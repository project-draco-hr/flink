{
  final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
  env.getConfig().enableObjectReuse();
  ParameterTool parameters=ParameterTool.fromArgs(args);
  int little_parallelism=parameters.getInt("little_parallelism",PARALLELISM_DEFAULT);
  DataSet ji;
switch (parameters.get("input","")) {
case "csv":
{
      String lineDelimiter=StringEscapeUtils.unescapeJava(parameters.get("input_line_delimiter",CsvOutputFormat.DEFAULT_LINE_DELIMITER));
      String fieldDelimiter=StringEscapeUtils.unescapeJava(parameters.get("input_field_delimiter",CsvOutputFormat.DEFAULT_FIELD_DELIMITER));
      GraphCsvReader reader=Graph.fromCsvReader(parameters.get("input_filename"),env).ignoreCommentsEdges("#").lineDelimiterEdges(lineDelimiter).fieldDelimiterEdges(fieldDelimiter);
switch (parameters.get("type","")) {
case "integer":
{
          ji=reader.keyType(LongValue.class).run(new org.apache.flink.graph.library.similarity.JaccardIndex<LongValue,NullValue,NullValue>().setLittleParallelism(little_parallelism));
        }
      break;
case "string":
{
      ji=reader.keyType(StringValue.class).run(new org.apache.flink.graph.library.similarity.JaccardIndex<StringValue,NullValue,NullValue>().setLittleParallelism(little_parallelism));
    }
  break;
default :
printUsage();
return;
}
}
break;
case "rmat":
{
int scale=parameters.getInt("scale",DEFAULT_SCALE);
int edgeFactor=parameters.getInt("edge_factor",DEFAULT_EDGE_FACTOR);
RandomGenerableFactory<JDKRandomGenerator> rnd=new JDKRandomGeneratorFactory();
long vertexCount=1L << scale;
long edgeCount=vertexCount * edgeFactor;
Graph<LongValue,NullValue,NullValue> graph=new RMatGraph<>(env,rnd,vertexCount,edgeCount).setParallelism(little_parallelism).generate();
boolean clipAndFlip=parameters.getBoolean("clip_and_flip",DEFAULT_CLIP_AND_FLIP);
if (scale > 32) {
ji=graph.run(new Simplify<LongValue,NullValue,NullValue>(clipAndFlip).setParallelism(little_parallelism)).run(new org.apache.flink.graph.library.similarity.JaccardIndex<LongValue,NullValue,NullValue>().setLittleParallelism(little_parallelism));
}
 else {
ji=graph.run(new TranslateGraphIds<LongValue,IntValue,NullValue,NullValue>(new LongValueToUnsignedIntValue()).setParallelism(little_parallelism)).run(new Simplify<IntValue,NullValue,NullValue>(clipAndFlip).setParallelism(little_parallelism)).run(new org.apache.flink.graph.library.similarity.JaccardIndex<IntValue,NullValue,NullValue>().setLittleParallelism(little_parallelism));
}
}
break;
default :
printUsage();
return;
}
switch (parameters.get("output","")) {
case "print":
for (Object e : ji.collect()) {
Result result=(Result)e;
System.out.println(result.toVerboseString());
}
break;
case "hash":
System.out.println(DataSetUtils.checksumHashCode(ji));
break;
case "csv":
String filename=parameters.get("output_filename");
String lineDelimiter=StringEscapeUtils.unescapeJava(parameters.get("output_line_delimiter",CsvOutputFormat.DEFAULT_LINE_DELIMITER));
String fieldDelimiter=StringEscapeUtils.unescapeJava(parameters.get("output_field_delimiter",CsvOutputFormat.DEFAULT_FIELD_DELIMITER));
ji.writeAsCsv(filename,lineDelimiter,fieldDelimiter);
env.execute();
break;
default :
printUsage();
return;
}
JobExecutionResult result=env.getLastJobExecutionResult();
NumberFormat nf=NumberFormat.getInstance();
System.out.println("Execution runtime: " + nf.format(result.getNetRuntime()) + " ms");
}
