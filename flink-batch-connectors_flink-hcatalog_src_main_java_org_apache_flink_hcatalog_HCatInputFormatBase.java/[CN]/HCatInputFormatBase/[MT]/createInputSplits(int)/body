{
  configuration.setInt("mapreduce.input.fileinputformat.split.minsize",minNumSplits);
  JobContext jobContext=null;
  try {
    jobContext=HadoopUtils.instantiateJobContext(configuration,new JobID());
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
  List<InputSplit> splits;
  try {
    splits=this.hCatInputFormat.getSplits(jobContext);
  }
 catch (  InterruptedException e) {
    throw new IOException("Could not get Splits.",e);
  }
  HadoopInputSplit[] hadoopInputSplits=new HadoopInputSplit[splits.size()];
  for (int i=0; i < hadoopInputSplits.length; i++) {
    hadoopInputSplits[i]=new HadoopInputSplit(i,splits.get(i),jobContext);
  }
  return hadoopInputSplits;
}
