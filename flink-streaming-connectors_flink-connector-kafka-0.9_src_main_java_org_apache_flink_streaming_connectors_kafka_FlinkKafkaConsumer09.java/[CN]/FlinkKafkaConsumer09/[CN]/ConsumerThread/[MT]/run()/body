{
  try {
    long pollTimeout=Long.parseLong(flinkKafkaConsumer.properties.getProperty(KEY_POLL_TIMEOUT,Long.toString(DEFAULT_POLL_TIMEOUT)));
    pollLoop:     while (running) {
      ConsumerRecords<byte[],byte[]> records;
synchronized (flinkKafkaConsumer.consumer) {
        try {
          records=flinkKafkaConsumer.consumer.poll(pollTimeout);
        }
 catch (        WakeupException we) {
          if (running) {
            throw we;
          }
          continue;
        }
      }
      for (int i=0; i < flinkKafkaConsumer.subscribedPartitions.size(); i++) {
        TopicPartition partition=flinkKafkaConsumer.subscribedPartitions.get(i);
        KafkaTopicPartition flinkPartition=flinkKafkaConsumer.subscribedPartitionsAsFlink.get(i);
        List<ConsumerRecord<byte[],byte[]>> partitionRecords=records.records(partition);
        for (int j=0; j < partitionRecords.size(); j++) {
          ConsumerRecord<byte[],byte[]> record=partitionRecords.get(j);
          T value=flinkKafkaConsumer.deserializer.deserialize(record.key(),record.value(),record.topic(),record.partition(),record.offset());
          if (flinkKafkaConsumer.deserializer.isEndOfStream(value)) {
            running=false;
            break pollLoop;
          }
synchronized (sourceContext.getCheckpointLock()) {
            flinkKafkaConsumer.processElement(sourceContext,flinkPartition,value,record.offset());
          }
        }
      }
    }
  }
 catch (  Throwable t) {
    if (running) {
      this.flinkKafkaConsumer.stopWithError(t);
    }
 else {
      LOG.debug("Stopped ConsumerThread threw exception",t);
    }
  }
 finally {
    try {
      flinkKafkaConsumer.consumer.close();
    }
 catch (    Throwable t) {
      LOG.warn("Error while closing consumer",t);
    }
  }
}
