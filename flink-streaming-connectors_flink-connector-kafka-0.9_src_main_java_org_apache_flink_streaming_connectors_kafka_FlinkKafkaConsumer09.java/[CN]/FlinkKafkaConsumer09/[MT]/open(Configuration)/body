{
  super.open(parameters);
  final int numConsumers=getRuntimeContext().getNumberOfParallelSubtasks();
  final int thisConsumerIndex=getRuntimeContext().getIndexOfThisSubtask();
  this.subscribedPartitionsAsFlink=assignPartitions(this.partitionInfos,numConsumers,thisConsumerIndex);
  if (this.subscribedPartitionsAsFlink.isEmpty()) {
    LOG.info("This consumer doesn't have any partitions assigned");
    this.partitionState=null;
    return;
  }
 else {
    StreamingRuntimeContext streamingRuntimeContext=(StreamingRuntimeContext)getRuntimeContext();
    properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,Boolean.toString(!streamingRuntimeContext.isCheckpointingEnabled()));
    this.consumer=new KafkaConsumer<>(properties);
  }
  subscribedPartitions=convertToKafkaTopicPartition(subscribedPartitionsAsFlink);
  this.consumer.assign(this.subscribedPartitions);
  if (!Boolean.valueOf(properties.getProperty(KEY_DISABLE_METRICS,"false"))) {
    Map<MetricName,? extends Metric> metrics=this.consumer.metrics();
    if (metrics == null) {
      LOG.info("Consumer implementation does not support metrics");
    }
 else {
      for (      Map.Entry<MetricName,? extends Metric> metric : metrics.entrySet()) {
        String name=consumerId + "-consumer-" + metric.getKey().name();
        DefaultKafkaMetricAccumulator kafkaAccumulator=DefaultKafkaMetricAccumulator.createFor(metric.getValue());
        if (kafkaAccumulator != null) {
          getRuntimeContext().addAccumulator(name,kafkaAccumulator);
        }
      }
    }
  }
  if (restoreToOffset != null) {
    for (    Map.Entry<KafkaTopicPartition,Long> info : restoreToOffset.entrySet()) {
      this.consumer.seek(new TopicPartition(info.getKey().getTopic(),info.getKey().getPartition()),info.getValue() + 1);
    }
    this.partitionState=restoreInfoFromCheckpoint();
  }
 else {
    this.partitionState=new HashMap<>();
  }
}
