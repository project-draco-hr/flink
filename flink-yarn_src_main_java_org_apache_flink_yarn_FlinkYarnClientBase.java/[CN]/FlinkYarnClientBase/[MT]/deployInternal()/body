{
  isReadyForDeployment();
  LOG.info("Using values:");
  LOG.info("\tTaskManager count = {}",taskManagerCount);
  LOG.info("\tJobManager memory = {}",jobManagerMemoryMb);
  LOG.info("\tTaskManager memory = {}",taskManagerMemoryMb);
  yarnApplication=yarnClient.createApplication();
  GetNewApplicationResponse appResponse=yarnApplication.getNewApplicationResponse();
  List<Tuple2<String,String>> dynProperties=CliFrontend.getDynamicProperties(dynamicPropertiesEncoded);
  for (  Tuple2<String,String> dynProperty : dynProperties) {
    flinkConfiguration.setString(dynProperty.f0,dynProperty.f1);
  }
  try {
    List<QueueInfo> queues=yarnClient.getAllQueues();
    if (queues.size() > 0 && this.yarnQueue != null) {
      boolean queueFound=false;
      for (      QueueInfo queue : queues) {
        if (queue.getQueueName().equals(this.yarnQueue)) {
          queueFound=true;
          break;
        }
      }
      if (!queueFound) {
        String queueNames="";
        for (        QueueInfo queue : queues) {
          queueNames+=queue.getQueueName() + ", ";
        }
        LOG.warn("The specified queue '" + this.yarnQueue + "' does not exist. "+ "Available queues: "+ queueNames);
      }
    }
 else {
      LOG.debug("The YARN cluster does not have any queues configured");
    }
  }
 catch (  Throwable e) {
    LOG.warn("Error while getting queue information from YARN: " + e.getMessage());
    if (LOG.isDebugEnabled()) {
      LOG.debug("Error details",e);
    }
  }
  final int yarnMinAllocationMB=conf.getInt("yarn.scheduler.minimum-allocation-mb",0);
  if (jobManagerMemoryMb < yarnMinAllocationMB || taskManagerMemoryMb < yarnMinAllocationMB) {
    LOG.warn("The JobManager or TaskManager memory is below the smallest possible YARN Container size. " + "The value of 'yarn.scheduler.minimum-allocation-mb' is '" + yarnMinAllocationMB + "'. Please increase the memory size."+ "YARN will allocate the smaller containers but the scheduler will account for the minimum-allocation-mb, maybe not all instances "+ "you requested will start.");
  }
  if (jobManagerMemoryMb < yarnMinAllocationMB) {
    jobManagerMemoryMb=yarnMinAllocationMB;
  }
  if (taskManagerMemoryMb < yarnMinAllocationMB) {
    taskManagerMemoryMb=yarnMinAllocationMB;
  }
  Resource maxRes=appResponse.getMaximumResourceCapability();
  final String NOTE="Please check the 'yarn.scheduler.maximum-allocation-mb' and the 'yarn.nodemanager.resource.memory-mb' configuration values\n";
  if (jobManagerMemoryMb > maxRes.getMemory()) {
    failSessionDuringDeployment();
    throw new YarnDeploymentException("The cluster does not have the requested resources for the JobManager available!\n" + "Maximum Memory: " + maxRes.getMemory() + "MB Requested: "+ jobManagerMemoryMb+ "MB. "+ NOTE);
  }
  if (taskManagerMemoryMb > maxRes.getMemory()) {
    failSessionDuringDeployment();
    throw new YarnDeploymentException("The cluster does not have the requested resources for the TaskManagers available!\n" + "Maximum Memory: " + maxRes.getMemory() + " Requested: "+ taskManagerMemoryMb+ "MB. "+ NOTE);
  }
  final String NOTE_RSC="\nThe Flink YARN client will try to allocate the YARN session, but maybe not all TaskManagers are " + "connecting from the beginning because the resources are currently not available in the cluster. " + "The allocation might take more time than usual because the Flink YARN client needs to wait until "+ "the resources become available.";
  int totalMemoryRequired=jobManagerMemoryMb + taskManagerMemoryMb * taskManagerCount;
  ClusterResourceDescription freeClusterMem=getCurrentFreeClusterResources(yarnClient);
  if (freeClusterMem.totalFreeMemory < totalMemoryRequired) {
    LOG.warn("This YARN session requires " + totalMemoryRequired + "MB of memory in the cluster. "+ "There are currently only "+ freeClusterMem.totalFreeMemory+ "MB available."+ NOTE_RSC);
  }
  if (taskManagerMemoryMb > freeClusterMem.containerLimit) {
    LOG.warn("The requested amount of memory for the TaskManagers (" + taskManagerMemoryMb + "MB) is more than "+ "the largest possible YARN container: "+ freeClusterMem.containerLimit+ NOTE_RSC);
  }
  if (jobManagerMemoryMb > freeClusterMem.containerLimit) {
    LOG.warn("The requested amount of memory for the JobManager (" + jobManagerMemoryMb + "MB) is more than "+ "the largest possible YARN container: "+ freeClusterMem.containerLimit+ NOTE_RSC);
  }
  int[] nmFree=Arrays.copyOf(freeClusterMem.nodeManagersFree,freeClusterMem.nodeManagersFree.length);
  if (!allocateResource(nmFree,jobManagerMemoryMb)) {
    LOG.warn("Unable to find a NodeManager that can fit the JobManager/Application master. " + "The JobManager requires " + jobManagerMemoryMb + "MB. NodeManagers available: "+ Arrays.toString(freeClusterMem.nodeManagersFree)+ NOTE_RSC);
  }
  for (int i=0; i < taskManagerCount; i++) {
    if (!allocateResource(nmFree,taskManagerMemoryMb)) {
      LOG.warn("There is not enough memory available in the YARN cluster. " + "The TaskManager(s) require " + taskManagerMemoryMb + "MB each. "+ "NodeManagers available: "+ Arrays.toString(freeClusterMem.nodeManagersFree)+ "\n"+ "After allocating the JobManager ("+ jobManagerMemoryMb+ "MB) and ("+ i+ "/"+ taskManagerCount+ ") TaskManagers, "+ "the following NodeManagers are available: "+ Arrays.toString(nmFree)+ NOTE_RSC);
    }
  }
  final String javaOpts=flinkConfiguration.getString(ConfigConstants.FLINK_JVM_OPTIONS,"");
  String logbackFile=configurationDirectory + File.separator + FlinkYarnSessionCli.CONFIG_FILE_LOGBACK_NAME;
  boolean hasLogback=new File(logbackFile).exists();
  String log4jFile=configurationDirectory + File.separator + FlinkYarnSessionCli.CONFIG_FILE_LOG4J_NAME;
  boolean hasLog4j=new File(log4jFile).exists();
  if (hasLogback) {
    shipFiles.add(new File(logbackFile));
  }
  if (hasLog4j) {
    shipFiles.add(new File(log4jFile));
  }
  ContainerLaunchContext amContainer=Records.newRecord(ContainerLaunchContext.class);
  String amCommand="$JAVA_HOME/bin/java" + " -Xmx" + Utils.calculateHeapSize(jobManagerMemoryMb,flinkConfiguration) + "M "+ javaOpts;
  if (hasLogback || hasLog4j) {
    amCommand+=" -Dlog.file=\"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/jobmanager.log\"";
    if (hasLogback) {
      amCommand+=" -Dlogback.configurationFile=file:" + FlinkYarnSessionCli.CONFIG_FILE_LOGBACK_NAME;
    }
    if (hasLog4j) {
      amCommand+=" -Dlog4j.configuration=file:" + FlinkYarnSessionCli.CONFIG_FILE_LOG4J_NAME;
    }
  }
  amCommand+=" " + getApplicationMasterClass().getName() + " "+ " 1>"+ ApplicationConstants.LOG_DIR_EXPANSION_VAR+ "/jobmanager.out"+ " 2>"+ ApplicationConstants.LOG_DIR_EXPANSION_VAR+ "/jobmanager.err";
  amContainer.setCommands(Collections.singletonList(amCommand));
  LOG.debug("Application Master start command: " + amCommand);
  final FileSystem fs=FileSystem.get(conf);
  if (!fs.getClass().getSimpleName().equals("GoogleHadoopFileSystem") && fs.getScheme().startsWith("file")) {
    LOG.warn("The file system scheme is '" + fs.getScheme() + "'. This indicates that the "+ "specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values."+ "The Flink YARN client needs to store its files in a distributed file system");
  }
  ApplicationSubmissionContext appContext=yarnApplication.getApplicationSubmissionContext();
  if (RecoveryMode.isHighAvailabilityModeActivated(flinkConfiguration)) {
    appContext.setMaxAppAttempts(flinkConfiguration.getInteger(ConfigConstants.YARN_APPLICATION_ATTEMPTS,YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));
    activateHighAvailabilitySupport(appContext);
  }
 else {
    appContext.setMaxAppAttempts(flinkConfiguration.getInteger(ConfigConstants.YARN_APPLICATION_ATTEMPTS,1));
  }
  final ApplicationId appId=appContext.getApplicationId();
  LocalResource appMasterJar=Records.newRecord(LocalResource.class);
  LocalResource flinkConf=Records.newRecord(LocalResource.class);
  Path remotePathJar=Utils.setupLocalResource(conf,fs,appId.toString(),flinkJarPath,appMasterJar,fs.getHomeDirectory());
  Path remotePathConf=Utils.setupLocalResource(conf,fs,appId.toString(),flinkConfigurationPath,flinkConf,fs.getHomeDirectory());
  Map<String,LocalResource> localResources=new HashMap<String,LocalResource>(2);
  localResources.put("flink.jar",appMasterJar);
  localResources.put("flink-conf.yaml",flinkConf);
  final Path[] paths=new Path[2 + shipFiles.size()];
  StringBuilder envShipFileList=new StringBuilder();
  for (int i=0; i < shipFiles.size(); i++) {
    File shipFile=shipFiles.get(i);
    LocalResource shipResources=Records.newRecord(LocalResource.class);
    Path shipLocalPath=new Path("file://" + shipFile.getAbsolutePath());
    paths[2 + i]=Utils.setupLocalResource(conf,fs,appId.toString(),shipLocalPath,shipResources,fs.getHomeDirectory());
    localResources.put(shipFile.getName(),shipResources);
    envShipFileList.append(paths[2 + i]);
    if (i + 1 < shipFiles.size()) {
      envShipFileList.append(',');
    }
  }
  paths[0]=remotePathJar;
  paths[1]=remotePathConf;
  sessionFilesDir=new Path(fs.getHomeDirectory(),".flink/" + appId.toString() + "/");
  FsPermission permission=new FsPermission(FsAction.ALL,FsAction.NONE,FsAction.NONE);
  fs.setPermission(sessionFilesDir,permission);
  Utils.setTokensFor(amContainer,paths,conf);
  amContainer.setLocalResources(localResources);
  fs.close();
  Map<String,String> appMasterEnv=new HashMap<String,String>();
  Utils.setupEnv(conf,appMasterEnv);
  appMasterEnv.put(FlinkYarnClient.ENV_TM_COUNT,String.valueOf(taskManagerCount));
  appMasterEnv.put(FlinkYarnClient.ENV_TM_MEMORY,String.valueOf(taskManagerMemoryMb));
  appMasterEnv.put(FlinkYarnClient.FLINK_JAR_PATH,remotePathJar.toString());
  appMasterEnv.put(FlinkYarnClient.ENV_APP_ID,appId.toString());
  appMasterEnv.put(FlinkYarnClient.ENV_CLIENT_HOME_DIR,fs.getHomeDirectory().toString());
  appMasterEnv.put(FlinkYarnClient.ENV_CLIENT_SHIP_FILES,envShipFileList.toString());
  appMasterEnv.put(FlinkYarnClient.ENV_CLIENT_USERNAME,UserGroupInformation.getCurrentUser().getShortUserName());
  appMasterEnv.put(FlinkYarnClient.ENV_SLOTS,String.valueOf(slots));
  appMasterEnv.put(FlinkYarnClient.ENV_DETACHED,String.valueOf(detached));
  appMasterEnv.put(FlinkYarnClient.ENV_STREAMING_MODE,String.valueOf(streamingMode));
  if (dynamicPropertiesEncoded != null) {
    appMasterEnv.put(FlinkYarnClient.ENV_DYNAMIC_PROPERTIES,dynamicPropertiesEncoded);
  }
  amContainer.setEnvironment(appMasterEnv);
  Resource capability=Records.newRecord(Resource.class);
  capability.setMemory(jobManagerMemoryMb);
  capability.setVirtualCores(1);
  String name;
  if (customName == null) {
    name="Flink session with " + taskManagerCount + " TaskManagers";
    if (detached) {
      name+=" (detached)";
    }
  }
 else {
    name=customName;
  }
  appContext.setApplicationName(name);
  appContext.setApplicationType("Apache Flink");
  appContext.setAMContainerSpec(amContainer);
  appContext.setResource(capability);
  if (yarnQueue != null) {
    appContext.setQueue(yarnQueue);
  }
  LOG.info("Submitting application master " + appId);
  yarnClient.submitApplication(appContext);
  LOG.info("Waiting for the cluster to be allocated");
  int waittime=0;
  loop:   while (true) {
    ApplicationReport report=yarnClient.getApplicationReport(appId);
    YarnApplicationState appState=report.getYarnApplicationState();
switch (appState) {
case FAILED:
case FINISHED:
case KILLED:
      throw new YarnDeploymentException("The YARN application unexpectedly switched to state " + appState + " during deployment. \n"+ "Diagnostics from YARN: "+ report.getDiagnostics()+ "\n"+ "If log aggregation is enabled on your cluster, use this command to further investigate the issue:\n"+ "yarn logs -applicationId "+ appId);
case RUNNING:
    LOG.info("YARN application has been deployed successfully.");
  break loop;
default :
LOG.info("Deploying cluster, current state " + appState);
if (waittime > 60000) {
LOG.info("Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster");
}
}
waittime+=1000;
Thread.sleep(1000);
}
return new FlinkYarnCluster(yarnClient,appId,conf,flinkConfiguration,sessionFilesDir,detached);
}
