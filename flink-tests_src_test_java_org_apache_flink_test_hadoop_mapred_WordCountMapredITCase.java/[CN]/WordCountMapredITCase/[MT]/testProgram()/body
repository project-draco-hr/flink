{
  final ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
  DataSet<Tuple2<LongWritable,Text>> input=env.readHadoopFile(new TextInputFormat(),LongWritable.class,Text.class,textPath);
  DataSet<String> text=input.map(new MapFunction<Tuple2<LongWritable,Text>,String>(){
    @Override public String map(    Tuple2<LongWritable,Text> value) throws Exception {
      return value.f1.toString();
    }
  }
);
  DataSet<Tuple2<String,Integer>> counts=text.flatMap(new Tokenizer()).groupBy(0).sum(1);
  DataSet<Tuple2<Text,LongWritable>> words=counts.map(new MapFunction<Tuple2<String,Integer>,Tuple2<Text,LongWritable>>(){
    @Override public Tuple2<Text,LongWritable> map(    Tuple2<String,Integer> value) throws Exception {
      return new Tuple2<Text,LongWritable>(new Text(value.f0),new LongWritable(value.f1));
    }
  }
);
  HadoopOutputFormat<Text,LongWritable> hadoopOutputFormat=new HadoopOutputFormat<Text,LongWritable>(new TextOutputFormat<Text,LongWritable>(),new JobConf());
  hadoopOutputFormat.getJobConf().set("mapred.textoutputformat.separator"," ");
  TextOutputFormat.setOutputPath(hadoopOutputFormat.getJobConf(),new Path(resultPath));
  words.output(hadoopOutputFormat);
  env.execute("Hadoop Compat WordCount");
}
