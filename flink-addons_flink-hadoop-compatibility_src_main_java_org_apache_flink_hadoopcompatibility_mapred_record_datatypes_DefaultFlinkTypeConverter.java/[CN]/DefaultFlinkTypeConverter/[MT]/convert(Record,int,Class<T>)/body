{
  if (hadoopType == LongWritable.class) {
    return (T)new LongWritable((flinkType.getField(pos,LongValue.class)).getValue());
  }
  if (hadoopType == org.apache.hadoop.io.Text.class) {
    return (T)new Text((flinkType.getField(pos,StringValue.class)).getValue());
  }
  if (hadoopType == org.apache.hadoop.io.IntWritable.class) {
    return (T)new IntWritable((flinkType.getField(pos,IntValue.class)).getValue());
  }
  if (hadoopType == org.apache.hadoop.io.FloatWritable.class) {
    return (T)new FloatWritable((flinkType.getField(pos,FloatValue.class)).getValue());
  }
  if (hadoopType == org.apache.hadoop.io.DoubleWritable.class) {
    return (T)new DoubleWritable((flinkType.getField(pos,DoubleValue.class)).getValue());
  }
  if (hadoopType == org.apache.hadoop.io.BooleanWritable.class) {
    return (T)new BooleanWritable((flinkType.getField(pos,BooleanValue.class)).getValue());
  }
  if (hadoopType == org.apache.hadoop.io.ByteWritable.class) {
    return (T)new ByteWritable((flinkType.getField(pos,ByteValue.class)).getValue());
  }
  throw new RuntimeException("Unable to convert Flink type (" + flinkType.getClass().getCanonicalName() + ") to Hadoop.");
}
