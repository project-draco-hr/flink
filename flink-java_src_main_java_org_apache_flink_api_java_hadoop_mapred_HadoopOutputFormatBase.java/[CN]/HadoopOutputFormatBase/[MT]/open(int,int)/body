{
  if (Integer.toString(taskNumber + 1).length() > 6) {
    throw new IOException("Task id too large.");
  }
  TaskAttemptID taskAttemptID=TaskAttemptID.forName("attempt__0000_r_" + String.format("%" + (6 - Integer.toString(taskNumber + 1).length()) + "s"," ").replace(" ","0") + Integer.toString(taskNumber + 1)+ "_0");
  this.jobConf.set("mapred.task.id",taskAttemptID.toString());
  this.jobConf.setInt("mapred.task.partition",taskNumber + 1);
  this.jobConf.set("mapreduce.task.attempt.id",taskAttemptID.toString());
  this.jobConf.setInt("mapreduce.task.partition",taskNumber + 1);
  try {
    this.context=HadoopUtils.instantiateTaskAttemptContext(this.jobConf,taskAttemptID);
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
  this.outputCommitter=this.jobConf.getOutputCommitter();
  try {
    this.jobContext=HadoopUtils.instantiateJobContext(this.jobConf,new JobID());
  }
 catch (  Exception e) {
    throw new RuntimeException(e);
  }
  this.outputCommitter.setupJob(jobContext);
  this.recordWriter=this.mapredOutputFormat.getRecordWriter(null,this.jobConf,Integer.toString(taskNumber + 1),new HadoopDummyProgressable());
}
