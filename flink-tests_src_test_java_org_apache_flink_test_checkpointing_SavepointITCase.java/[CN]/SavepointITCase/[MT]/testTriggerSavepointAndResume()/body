{
  int numTaskManagers=2;
  int numSlotsPerTaskManager=2;
  int parallelism=numTaskManagers * numSlotsPerTaskManager;
  final Deadline deadline=new FiniteDuration(5,TimeUnit.MINUTES).fromNow();
  final int numberOfCompletedCheckpoints=10;
  final File tmpDir=CommonTestUtils.createTempDirectory();
  LOG.info("Created temporary directory: " + tmpDir + ".");
  ForkableFlinkMiniCluster flink=null;
  try {
    ActorSystem testActorSystem=AkkaUtils.createDefaultActorSystem();
    final Configuration config=new Configuration();
    config.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER,numTaskManagers);
    config.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS,numSlotsPerTaskManager);
    final File checkpointDir=new File(tmpDir,"checkpoints");
    final File savepointDir=new File(tmpDir,"savepoints");
    if (!checkpointDir.mkdir() || !savepointDir.mkdirs()) {
      fail("Test setup failed: failed to create temporary directories.");
    }
    LOG.info("Created temporary checkpoint directory: " + checkpointDir + ".");
    LOG.info("Created temporary savepoint directory: " + savepointDir + ".");
    config.setString(ConfigConstants.STATE_BACKEND,"filesystem");
    config.setString(FsStateBackendFactory.CHECKPOINT_DIRECTORY_URI_CONF_KEY,checkpointDir.toURI().toString());
    config.setString(SavepointStoreFactory.SAVEPOINT_BACKEND_KEY,"filesystem");
    config.setString(SavepointStoreFactory.SAVEPOINT_DIRECTORY_KEY,savepointDir.toURI().toString());
    LOG.info("Flink configuration: " + config + ".");
    flink=new ForkableFlinkMiniCluster(config);
    LOG.info("Starting Flink cluster.");
    flink.start();
    LOG.info("Retrieving JobManager.");
    ActorGateway jobManager=Await.result(flink.leaderGateway().future(),deadline.timeLeft());
    LOG.info("JobManager: " + jobManager + ".");
    final JobGraph jobGraph=createJobGraph(parallelism,0,1000,1000);
    final JobID jobId=jobGraph.getJobID();
    InfiniteTestSource.CheckpointCompleteLatch=new CountDownLatch(numberOfCompletedCheckpoints);
    LOG.info("Submitting job " + jobGraph.getJobID() + " in detached mode.");
    flink.submitJobDetached(jobGraph);
    LOG.info("Waiting for " + numberOfCompletedCheckpoints + " checkpoint complete notifications.");
    InfiniteTestSource.CheckpointCompleteLatch.await();
    LOG.info("Received all " + numberOfCompletedCheckpoints + " checkpoint complete notifications.");
    LOG.info("Triggering a savepoint.");
    Future<Object> savepointPathFuture=jobManager.ask(new TriggerSavepoint(jobId),deadline.timeLeft());
    final String savepointPath=((TriggerSavepointSuccess)Await.result(savepointPathFuture,deadline.timeLeft())).savepointPath();
    LOG.info("Retrieved savepoint path: " + savepointPath + ".");
    LOG.info("Requesting the savepoint.");
    Future<Object> savepointFuture=jobManager.ask(new RequestSavepoint(savepointPath),deadline.timeLeft());
    SavepointV0 savepoint=(SavepointV0)((ResponseSavepoint)Await.result(savepointFuture,deadline.timeLeft())).savepoint();
    LOG.info("Retrieved savepoint: " + savepointPath + ".");
    LOG.info("Shutting down Flink cluster.");
    flink.shutdown();
    String errMsg="Checkpoints directory not cleaned up properly.";
    File[] files=checkpointDir.listFiles();
    if (files != null) {
      assertEquals(errMsg,1,files.length);
    }
 else {
      fail(errMsg);
    }
    errMsg="Savepoints directory cleaned up.";
    files=savepointDir.listFiles();
    if (files != null) {
      assertEquals(errMsg,1,files.length);
    }
 else {
      fail(errMsg);
    }
    LOG.info("Restarting Flink cluster.");
    flink.start();
    LOG.info("Retrieving JobManager.");
    jobManager=Await.result(flink.leaderGateway().future(),deadline.timeLeft());
    LOG.info("JobManager: " + jobManager + ".");
    final Throwable[] error=new Throwable[1];
    final ForkableFlinkMiniCluster finalFlink=flink;
    final Multimap<JobVertexID,TaskDeploymentDescriptor> tdds=HashMultimap.create();
    new JavaTestKit(testActorSystem){
{
        new Within(deadline.timeLeft()){
          @Override protected void run(){
            try {
              for (              ActorRef taskManager : finalFlink.getTaskManagersAsJava()) {
                taskManager.tell(new TestingTaskManagerMessages.RegisterSubmitTaskListener(jobId),getTestActor());
              }
              jobGraph.setSavepointPath(savepointPath);
              LOG.info("Resubmitting job " + jobGraph.getJobID() + " with "+ "savepoint path "+ savepointPath+ " in detached mode.");
              finalFlink.submitJobDetached(jobGraph);
              int numTasks=0;
              for (              JobVertex jobVertex : jobGraph.getVertices()) {
                numTasks+=jobVertex.getParallelism();
              }
              LOG.info("Gathering " + numTasks + " submitted "+ "TaskDeploymentDescriptor instances.");
              for (int i=0; i < numTasks; i++) {
                ResponseSubmitTaskListener resp=(ResponseSubmitTaskListener)expectMsgAnyClassOf(getRemainingTime(),ResponseSubmitTaskListener.class);
                TaskDeploymentDescriptor tdd=resp.tdd();
                LOG.info("Received: " + tdd.toString() + ".");
                tdds.put(tdd.getVertexID(),tdd);
              }
            }
 catch (            Throwable t) {
              error[0]=t;
            }
          }
        }
;
      }
    }
;
    errMsg="Error during gathering of TaskDeploymentDescriptors";
    assertNull(errMsg,error[0]);
    for (    TaskState taskState : savepoint.getTaskStates()) {
      Collection<TaskDeploymentDescriptor> taskTdds=tdds.get(taskState.getJobVertexID());
      errMsg="Missing task for savepoint state for operator " + taskState.getJobVertexID() + ".";
      assertTrue(errMsg,taskTdds.size() > 0);
      assertEquals(taskState.getNumberCollectedStates(),taskTdds.size());
      for (      TaskDeploymentDescriptor tdd : taskTdds) {
        SubtaskState subtaskState=taskState.getState(tdd.getIndexInSubtaskGroup());
        assertNotNull(subtaskState);
        errMsg="Initial operator state mismatch.";
        assertEquals(errMsg,subtaskState.getState(),tdd.getOperatorState());
      }
    }
    LOG.info("Cancelling job " + jobId + ".");
    jobManager.tell(new CancelJob(jobId));
    LOG.info("Disposing savepoint " + savepointPath + ".");
    Future<Object> disposeFuture=jobManager.ask(new DisposeSavepoint(savepointPath,Option.<List<BlobKey>>empty()),deadline.timeLeft());
    errMsg="Failed to dispose savepoint " + savepointPath + ".";
    Object resp=Await.result(disposeFuture,deadline.timeLeft());
    assertTrue(errMsg,resp.getClass() == getDisposeSavepointSuccess().getClass());
    List<File> checkpointFiles=new ArrayList<>();
    for (    TaskState stateForTaskGroup : savepoint.getTaskStates()) {
      for (      SubtaskState subtaskState : stateForTaskGroup.getStates()) {
        StreamTaskStateList taskStateList=(StreamTaskStateList)subtaskState.getState().deserializeValue(ClassLoader.getSystemClassLoader());
        for (        StreamTaskState taskState : taskStateList.getState(ClassLoader.getSystemClassLoader())) {
          AbstractFileStateHandle fsState=(AbstractFileStateHandle)taskState.getFunctionState();
          checkpointFiles.add(new File(fsState.getFilePath().toUri()));
        }
      }
    }
    for (    File f : checkpointFiles) {
      errMsg="Checkpoint file " + f + " not cleaned up properly.";
      assertFalse(errMsg,f.exists());
    }
    if (checkpointFiles.size() > 0) {
      File parent=checkpointFiles.get(0).getParentFile();
      errMsg="Checkpoint parent directory " + parent + " not cleaned up properly.";
      assertFalse(errMsg,parent.exists());
    }
    errMsg="Savepoints directory not cleaned up properly: " + Arrays.toString(savepointDir.listFiles()) + ".";
    assertEquals(errMsg,0,savepointDir.listFiles().length);
  }
  finally {
    if (flink != null) {
      flink.shutdown();
    }
    if (tmpDir != null) {
      FileUtils.deleteDirectory(tmpDir);
    }
  }
}
