{
  super.open(parameters);
  final int numConsumers=getRuntimeContext().getNumberOfParallelSubtasks();
  final int thisConsumerIndex=getRuntimeContext().getIndexOfThisSubtask();
  subscribedPartitions=assignPartitions(this.partitionInfos,numConsumers,thisConsumerIndex);
  if (LOG.isInfoEnabled()) {
    LOG.info("Kafka consumer {} will read partitions {} out of partitions {}",thisConsumerIndex,KafkaTopicPartitionLeader.toString(subscribedPartitions),this.partitionInfos.size());
  }
  if (subscribedPartitions.isEmpty()) {
    LOG.info("Kafka consumer {} has no partitions (empty source)",thisConsumerIndex);
    return;
  }
switch (fetcherType) {
case NEW_HIGH_LEVEL:
    throw new UnsupportedOperationException("Currently unsupported");
case LEGACY_LOW_LEVEL:
  fetcher=new LegacyFetcher(this.subscribedPartitions,props,getRuntimeContext().getTaskName());
break;
default :
throw new RuntimeException("Requested unknown fetcher " + fetcher);
}
switch (offsetStore) {
case FLINK_ZOOKEEPER:
offsetHandler=new ZookeeperOffsetHandler(props);
break;
case KAFKA:
throw new Exception("Kafka offset handler cannot work with legacy fetcher");
default :
throw new RuntimeException("Requested unknown offset store " + offsetStore);
}
committedOffsets=new HashMap<>();
if (restoreToOffset != null) {
if (LOG.isInfoEnabled()) {
LOG.info("Consumer {} is restored from previous checkpoint: {}",thisConsumerIndex,KafkaTopicPartition.toString(restoreToOffset));
}
for (Map.Entry<KafkaTopicPartition,Long> restorePartition : restoreToOffset.entrySet()) {
fetcher.seek(restorePartition.getKey(),restorePartition.getValue() + 1);
}
this.lastOffsets=restoreToOffset;
restoreToOffset=null;
}
 else {
lastOffsets=new HashMap<>();
offsetHandler.seekFetcherToInitialOffsets(subscribedPartitions,fetcher);
}
}
