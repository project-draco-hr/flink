{
  this.offsetStore=checkNotNull(offsetStore);
  this.fetcherType=checkNotNull(fetcherType);
  if (fetcherType == FetcherType.NEW_HIGH_LEVEL) {
    throw new UnsupportedOperationException("The fetcher for Kafka 0.8.3 / 0.9.0 is not yet " + "supported in Flink");
  }
  if (offsetStore == OffsetStore.KAFKA && fetcherType == FetcherType.LEGACY_LOW_LEVEL) {
    throw new IllegalArgumentException("The Kafka offset handler cannot be used together with the old low-level fetcher.");
  }
  checkNotNull(topics,"topics");
  this.props=checkNotNull(props,"props");
  this.deserializer=checkNotNull(deserializer,"valueDeserializer");
  if (offsetStore == OffsetStore.FLINK_ZOOKEEPER) {
    validateZooKeeperConfig(props);
  }
  this.partitionInfos=getPartitionsForTopic(topics,props);
  if (partitionInfos.size() == 0) {
    throw new RuntimeException("Unable to retrieve any partitions for the requested topics " + topics.toString() + "."+ "Please check previous log entries");
  }
  if (LOG.isInfoEnabled()) {
    Map<String,Integer> countPerTopic=new HashMap<>();
    for (    KafkaTopicPartitionLeader partition : partitionInfos) {
      Integer count=countPerTopic.get(partition.getTopicPartition().getTopic());
      if (count == null) {
        count=1;
      }
 else {
        count++;
      }
      countPerTopic.put(partition.getTopicPartition().getTopic(),count);
    }
    StringBuilder sb=new StringBuilder();
    for (    Map.Entry<String,Integer> e : countPerTopic.entrySet()) {
      sb.append(e.getKey()).append(" (").append(e.getValue()).append("), ");
    }
    LOG.info("Consumer is going to read the following topics (with number of partitions): ",sb.toString());
  }
}
