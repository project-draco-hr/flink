{
  TypeInformation<Tuple2<Integer,Integer>> resultType=TypeInfoParser.parse("Tuple2<Integer, Integer>");
  DataStream<Tuple2<Integer,Integer>> stream=env.addSource(new RichParallelSourceFunction<Tuple2<Integer,Integer>>(){
    private boolean running=true;
    @Override public void run(    SourceContext<Tuple2<Integer,Integer>> ctx) throws Exception {
      int cnt=0;
      int partition=getRuntimeContext().getIndexOfThisSubtask();
      while (running && cnt < numElements) {
        ctx.collect(new Tuple2<>(partition,cnt));
        cnt++;
      }
    }
    @Override public void cancel(){
      running=false;
    }
  }
).setParallelism(parallelism);
  stream.addSink(new FlinkKafkaProducer<>(topicName,new TypeInformationSerializationSchema<>(resultType,env.getConfig()),FlinkKafkaProducer.getPropertiesFromBrokerList(brokerConnectionStrings),new Tuple2Partitioner(parallelism))).setParallelism(parallelism);
  env.execute("Write sequence");
  LOG.info("Finished writing sequence");
}
