{
  final int NUM_TOPICS=5;
  final int NUM_ELEMENTS=20;
  final StreamExecutionEnvironment env=StreamExecutionEnvironment.createRemoteEnvironment("localhost",flinkPort);
  final List<String> topics=new ArrayList<>();
  for (int i=0; i < NUM_TOPICS; i++) {
    final String topic="topic-" + i;
    topics.add(topic);
    createTestTopic(topic,i + 1,1);
    writeSequence(env,topic,NUM_ELEMENTS,i + 1);
  }
  List<KafkaTopicPartitionLeader> topicPartitions=FlinkKafkaConsumer082.getPartitionsForTopic(topics,standardProps);
  Assert.assertEquals((NUM_TOPICS * (NUM_TOPICS + 1)) / 2,topicPartitions.size());
  KeyedDeserializationSchema<Tuple3<Integer,Integer,String>> readSchema=new Tuple2WithTopicDeserializationSchema(env.getConfig());
  DataStreamSource<Tuple3<Integer,Integer,String>> stream=env.addSource(getConsumer(topics,readSchema,standardProps));
  stream.flatMap(new FlatMapFunction<Tuple3<Integer,Integer,String>,Integer>(){
    Map<String,Integer> countPerTopic=new HashMap<>(NUM_TOPICS);
    @Override public void flatMap(    Tuple3<Integer,Integer,String> value,    Collector<Integer> out) throws Exception {
      Integer count=countPerTopic.get(value.f2);
      if (count == null) {
        count=1;
      }
 else {
        count++;
      }
      countPerTopic.put(value.f2,count);
      for (      Map.Entry<String,Integer> el : countPerTopic.entrySet()) {
        if (el.getValue() < NUM_ELEMENTS) {
          break;
        }
        if (el.getValue() > NUM_ELEMENTS) {
          throw new RuntimeException("There is a failure in the test. I've read " + el.getValue() + " from topic "+ el.getKey());
        }
      }
      throw new SuccessException();
    }
  }
).setParallelism(1);
  tryExecute(env,"Count elements from the topics");
  for (int i=0; i < NUM_TOPICS; i++) {
    final String topic="topic-" + i;
    deleteTestTopic(topic);
  }
}
