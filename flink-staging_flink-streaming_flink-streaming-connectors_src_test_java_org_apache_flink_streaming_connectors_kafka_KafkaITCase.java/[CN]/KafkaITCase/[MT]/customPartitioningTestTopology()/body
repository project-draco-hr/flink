{
  LOG.info("Starting KafkaITCase.customPartitioningTestTopology()");
  String topic="customPartitioningTestTopic";
  createTestTopic(topic,3);
  final StreamExecutionEnvironment env=StreamExecutionEnvironment.createLocalEnvironment(1);
  DataStreamSource<Tuple2<Long,String>> consuming=env.addSource(new PersistentKafkaSource<Tuple2<Long,String>>(zookeeperConnectionString,topic,new TupleSerializationSchema(),5000,100,Offset.FROM_BEGINNING));
  consuming.addSink(new SinkFunction<Tuple2<Long,String>>(){
    int start=-1;
    BitSet validator=new BitSet(101);
    boolean gotPartition1=false;
    boolean gotPartition2=false;
    boolean gotPartition3=false;
    @Override public void invoke(    Tuple2<Long,String> value) throws Exception {
      LOG.info("Got " + value);
      String[] sp=value.f1.split("-");
      int v=Integer.parseInt(sp[1]);
      assertEquals(value.f0 - 1000,(long)v);
switch (v) {
case 9:
        gotPartition1=true;
      break;
case 19:
    gotPartition2=true;
  break;
case 99:
gotPartition3=true;
break;
}
if (start == -1) {
start=v;
}
Assert.assertFalse("Received tuple twice",validator.get(v - start));
validator.set(v - start);
if (gotPartition1 && gotPartition2 && gotPartition3) {
int nc;
if ((nc=validator.nextClearBit(0)) != 100) {
throw new RuntimeException("The bitset was not set to 1 on all elements. Next clear:" + nc + " Set: "+ validator);
}
throw new SuccessException();
}
}
}
);
DataStream<Tuple2<Long,String>> stream=env.addSource(new SourceFunction<Tuple2<Long,String>>(){
private static final long serialVersionUID=1L;
boolean running=true;
@Override public void run(Collector<Tuple2<Long,String>> collector) throws Exception {
LOG.info("Starting source.");
int cnt=0;
while (running) {
collector.collect(new Tuple2<Long,String>(1000L + cnt,"kafka-" + cnt++));
try {
Thread.sleep(100);
}
 catch (InterruptedException ignored) {
}
}
}
@Override public void cancel(){
LOG.info("Source got cancel()");
running=false;
}
}
);
stream.addSink(new KafkaSink<Tuple2<Long,String>>(zookeeperConnectionString,topic,new TupleSerializationSchema(),new CustomPartitioner()));
try {
env.setParallelism(1);
env.execute();
}
 catch (JobExecutionException good) {
Throwable t=good.getCause();
int limit=0;
while (!(t instanceof SuccessException)) {
t=t.getCause();
if (limit++ == 20) {
throw good;
}
}
assertTrue(partitionerHasBeenCalled);
}
LOG.info("Finished KafkaITCase.customPartitioningTestTopology()");
}
