{
  LOG.info("Starting KafkaITCase.bigRecordTestTopology()");
  String topic="bigRecordTestTopic";
  createTestTopic(topic,1,1);
  final StreamExecutionEnvironment env=StreamExecutionEnvironment.createLocalEnvironment(1);
  Utils.TypeInformationSerializationSchema<Tuple2<Long,byte[]>> serSchema=new Utils.TypeInformationSerializationSchema<Tuple2<Long,byte[]>>(new Tuple2<Long,byte[]>(0L,new byte[]{0}),env.getConfig());
  Properties consumerProps=new Properties();
  consumerProps.setProperty("fetch.message.max.bytes",Integer.toString(1024 * 1024 * 30));
  consumerProps.setProperty("zookeeper.connect",zookeeperConnectionString);
  consumerProps.setProperty("group.id","test");
  ConsumerConfig cc=new ConsumerConfig(consumerProps);
  DataStreamSource<Tuple2<Long,byte[]>> consuming=env.addSource(new PersistentKafkaSource<Tuple2<Long,byte[]>>(topic,serSchema,Offset.FROM_BEGINNING,cc));
  consuming.addSink(new SinkFunction<Tuple2<Long,byte[]>>(){
    int elCnt=0;
    @Override public void invoke(    Tuple2<Long,byte[]> value) throws Exception {
      elCnt++;
      if (value.f0 == -1) {
        if (elCnt == 11) {
          throw new SuccessException();
        }
 else {
          throw new RuntimeException("There have been " + elCnt + " elements");
        }
      }
      if (elCnt > 10) {
        throw new RuntimeException("More than 10 elements seen: " + elCnt);
      }
    }
  }
);
  DataStream<Tuple2<Long,byte[]>> stream=env.addSource(new RichSourceFunction<Tuple2<Long,byte[]>>(){
    private static final long serialVersionUID=1L;
    boolean running=true;
    @Override public void open(    Configuration parameters) throws Exception {
      super.open(parameters);
    }
    @Override public void run(    Collector<Tuple2<Long,byte[]>> collector) throws Exception {
      LOG.info("Starting source.");
      long cnt=0;
      Random rnd=new Random(1337);
      while (running) {
        byte[] wl=new byte[Math.abs(rnd.nextInt(1024 * 1024 * 30))];
        collector.collect(new Tuple2<Long,byte[]>(cnt++,wl));
        LOG.info("Emitted cnt=" + (cnt - 1) + " with byte.length = "+ wl.length);
        try {
          Thread.sleep(100);
        }
 catch (        InterruptedException ignored) {
        }
        if (cnt == 10) {
          collector.collect(new Tuple2<Long,byte[]>(-1L,new byte[]{1}));
          running=false;
        }
      }
    }
    @Override public void cancel(){
      LOG.info("Source got cancel()");
      running=false;
    }
  }
);
  stream.addSink(new KafkaSink<Tuple2<Long,byte[]>>(zookeeperConnectionString,topic,new Utils.TypeInformationSerializationSchema<Tuple2<Long,byte[]>>(new Tuple2<Long,byte[]>(0L,new byte[]{0}),env.getConfig())));
  try {
    env.setParallelism(1);
    env.execute();
  }
 catch (  JobExecutionException good) {
    Throwable t=good.getCause();
    int limit=0;
    while (!(t instanceof SuccessException)) {
      t=t.getCause();
      if (limit++ == 20) {
        LOG.warn("Test failed with exception",good);
        Assert.fail("Test failed with: " + good.getMessage());
      }
    }
  }
  LOG.info("Finished KafkaITCase.bigRecordTestTopology()");
}
