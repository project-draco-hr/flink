{
  try {
    final int[] newPartitionIDs={4,52,17,1,2,3,89,42,31,127,14};
    List<KafkaTopicPartitionLeader> newPartitions=new ArrayList<>();
    for (    int p : newPartitionIDs) {
      KafkaTopicPartitionLeader part=new KafkaTopicPartitionLeader(new KafkaTopicPartition("test-topic",p),fake);
      newPartitions.add(part);
    }
    List<KafkaTopicPartitionLeader> initialPartitions=newPartitions.subList(0,7);
    final Set<KafkaTopicPartitionLeader> allNewPartitions=new HashSet<>(newPartitions);
    final Set<KafkaTopicPartitionLeader> allInitialPartitions=new HashSet<>(initialPartitions);
    final int numConsumers=3;
    final int minInitialPartitionsPerConsumer=initialPartitions.size() / numConsumers;
    final int maxInitialPartitionsPerConsumer=initialPartitions.size() / numConsumers + 1;
    final int minNewPartitionsPerConsumer=newPartitions.size() / numConsumers;
    final int maxNewPartitionsPerConsumer=newPartitions.size() / numConsumers + 1;
    List<KafkaTopicPartitionLeader> parts1=FlinkKafkaConsumerBase.assignPartitions(initialPartitions,numConsumers,0);
    List<KafkaTopicPartitionLeader> parts2=FlinkKafkaConsumerBase.assignPartitions(initialPartitions,numConsumers,1);
    List<KafkaTopicPartitionLeader> parts3=FlinkKafkaConsumerBase.assignPartitions(initialPartitions,numConsumers,2);
    assertNotNull(parts1);
    assertNotNull(parts2);
    assertNotNull(parts3);
    assertTrue(parts1.size() >= minInitialPartitionsPerConsumer);
    assertTrue(parts1.size() <= maxInitialPartitionsPerConsumer);
    assertTrue(parts2.size() >= minInitialPartitionsPerConsumer);
    assertTrue(parts2.size() <= maxInitialPartitionsPerConsumer);
    assertTrue(parts3.size() >= minInitialPartitionsPerConsumer);
    assertTrue(parts3.size() <= maxInitialPartitionsPerConsumer);
    for (    KafkaTopicPartitionLeader p : parts1) {
      assertTrue(allInitialPartitions.remove(p));
    }
    for (    KafkaTopicPartitionLeader p : parts2) {
      assertTrue(allInitialPartitions.remove(p));
    }
    for (    KafkaTopicPartitionLeader p : parts3) {
      assertTrue(allInitialPartitions.remove(p));
    }
    assertTrue(allInitialPartitions.isEmpty());
    List<KafkaTopicPartitionLeader> parts1new=FlinkKafkaConsumerBase.assignPartitions(newPartitions,numConsumers,0);
    List<KafkaTopicPartitionLeader> parts2new=FlinkKafkaConsumerBase.assignPartitions(newPartitions,numConsumers,1);
    List<KafkaTopicPartitionLeader> parts3new=FlinkKafkaConsumerBase.assignPartitions(newPartitions,numConsumers,2);
    assertTrue(parts1new.size() > parts1.size());
    assertTrue(parts2new.size() > parts2.size());
    assertTrue(parts3new.size() > parts3.size());
    assertTrue(parts1new.containsAll(parts1));
    assertTrue(parts2new.containsAll(parts2));
    assertTrue(parts3new.containsAll(parts3));
    assertTrue(parts1new.size() >= minNewPartitionsPerConsumer);
    assertTrue(parts1new.size() <= maxNewPartitionsPerConsumer);
    assertTrue(parts2new.size() >= minNewPartitionsPerConsumer);
    assertTrue(parts2new.size() <= maxNewPartitionsPerConsumer);
    assertTrue(parts3new.size() >= minNewPartitionsPerConsumer);
    assertTrue(parts3new.size() <= maxNewPartitionsPerConsumer);
    for (    KafkaTopicPartitionLeader p : parts1new) {
      assertTrue(allNewPartitions.remove(p));
    }
    for (    KafkaTopicPartitionLeader p : parts2new) {
      assertTrue(allNewPartitions.remove(p));
    }
    for (    KafkaTopicPartitionLeader p : parts3new) {
      assertTrue(allNewPartitions.remove(p));
    }
    assertTrue(allNewPartitions.isEmpty());
  }
 catch (  Exception e) {
    e.printStackTrace();
    fail(e.getMessage());
  }
}
