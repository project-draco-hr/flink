{
  if (UserGroupInformation.isSecurityEnabled()) {
    throw new RuntimeException("Flink YARN client does not have security support right now." + "File a bug, we will fix it asap");
  }
  Options options=new Options();
  options.addOption(VERBOSE);
  options.addOption(FLINK_CONF_DIR);
  options.addOption(FLINK_JAR);
  options.addOption(JM_MEMORY);
  options.addOption(TM_MEMORY);
  options.addOption(TM_CORES);
  options.addOption(CONTAINER);
  options.addOption(GEN_CONF);
  options.addOption(QUEUE);
  options.addOption(QUERY);
  options.addOption(SHIP_PATH);
  options.addOption(SLOTS);
  options.addOption(DYNAMIC_PROPERTIES);
  CommandLineParser parser=new PosixParser();
  CommandLine cmd=null;
  try {
    cmd=parser.parse(options,args);
  }
 catch (  MissingOptionException moe) {
    System.out.println(moe.getMessage());
    printUsage();
    System.exit(1);
  }
  Path localJarPath;
  if (cmd.hasOption(FLINK_JAR.getOpt())) {
    String userPath=cmd.getOptionValue(FLINK_JAR.getOpt());
    if (!userPath.startsWith("file://")) {
      userPath="file://" + userPath;
    }
    localJarPath=new Path(userPath);
  }
 else {
    localJarPath=new Path("file://" + Client.class.getProtectionDomain().getCodeSource().getLocation().getPath());
  }
  if (cmd.hasOption(GEN_CONF.getOpt())) {
    LOG.info("Placing default configuration in current directory");
    File outFile=generateDefaultConf(localJarPath);
    LOG.info("File written to " + outFile.getAbsolutePath());
    System.exit(0);
  }
  Path confPath=null;
  String confDirPath="";
  if (cmd.hasOption(FLINK_CONF_DIR.getOpt())) {
    confDirPath=cmd.getOptionValue(FLINK_CONF_DIR.getOpt()) + "/";
    File confFile=new File(confDirPath + CONFIG_FILE_NAME);
    if (!confFile.exists()) {
      LOG.error("Unable to locate configuration file in " + confFile);
      System.exit(1);
    }
    confPath=new Path(confFile.getAbsolutePath());
  }
 else {
    System.out.println("No configuration file has been specified");
    File currDir=new File(".");
    File[] candidates=currDir.listFiles(new FilenameFilter(){
      @Override public boolean accept(      final File dir,      final String name){
        return name != null && name.endsWith(".yaml");
      }
    }
);
    if (candidates == null || candidates.length == 0) {
      System.out.println("No configuration file has been found in current directory.\n" + "Copying default.");
      File outFile=generateDefaultConf(localJarPath);
      confPath=new Path(outFile.toURI());
    }
 else {
      if (candidates.length > 1) {
        System.out.println("Multiple .yaml configuration files were found in the current directory\n" + "Please specify one explicitly");
        System.exit(1);
      }
 else       if (candidates.length == 1) {
        confPath=new Path(candidates[0].toURI());
      }
    }
  }
  List<File> shipFiles=new ArrayList<File>();
  if (cmd.hasOption(SHIP_PATH.getOpt())) {
    String shipPath=cmd.getOptionValue(SHIP_PATH.getOpt());
    File shipDir=new File(shipPath);
    if (shipDir.isDirectory()) {
      shipFiles=new ArrayList<File>(Arrays.asList(shipDir.listFiles(new FilenameFilter(){
        @Override public boolean accept(        File dir,        String name){
          return !(name.equals(".") || name.equals(".."));
        }
      }
)));
    }
 else {
      LOG.warn("Ship directory is not a directory!");
    }
  }
  boolean hasLogback=false;
  boolean hasLog4j=false;
  if (confDirPath.length() > 0) {
    File logback=new File(confDirPath + "/logback.xml");
    if (logback.exists()) {
      shipFiles.add(logback);
      hasLogback=true;
    }
    File log4j=new File(confDirPath + "/log4j.properties");
    if (log4j.exists()) {
      shipFiles.add(log4j);
      hasLog4j=true;
    }
  }
  String queue="default";
  if (cmd.hasOption(QUEUE.getOpt())) {
    queue=cmd.getOptionValue(QUEUE.getOpt());
  }
  int jmMemory=512;
  if (cmd.hasOption(JM_MEMORY.getOpt())) {
    jmMemory=Integer.valueOf(cmd.getOptionValue(JM_MEMORY.getOpt()));
  }
  if (jmMemory < MIN_JM_MEMORY) {
    System.out.println("The JobManager memory is below the minimum required memory amount " + "of " + MIN_JM_MEMORY + " MB");
    System.exit(1);
  }
  int tmMemory=1024;
  if (cmd.hasOption(TM_MEMORY.getOpt())) {
    tmMemory=Integer.valueOf(cmd.getOptionValue(TM_MEMORY.getOpt()));
  }
  if (tmMemory < MIN_TM_MEMORY) {
    System.out.println("The TaskManager memory is below the minimum required memory amount " + "of " + MIN_TM_MEMORY + " MB");
    System.exit(1);
  }
  if (cmd.hasOption(SLOTS.getOpt())) {
    slots=Integer.valueOf(cmd.getOptionValue(SLOTS.getOpt()));
  }
  String[] dynamicProperties=null;
  if (cmd.hasOption(DYNAMIC_PROPERTIES.getOpt())) {
    dynamicProperties=cmd.getOptionValues(DYNAMIC_PROPERTIES.getOpt());
  }
  String dynamicPropertiesEncoded=StringUtils.join(dynamicProperties,CliFrontend.YARN_DYNAMIC_PROPERTIES_SEPARATOR);
  int tmCores=1;
  if (cmd.hasOption(TM_CORES.getOpt())) {
    tmCores=Integer.valueOf(cmd.getOptionValue(TM_CORES.getOpt()));
  }
  Utils.getFlinkConfiguration(confPath.toUri().getPath());
  int jmPort=GlobalConfiguration.getInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY,0);
  if (jmPort == 0) {
    LOG.warn("Unable to find job manager port in configuration!");
    jmPort=ConfigConstants.DEFAULT_JOB_MANAGER_IPC_PORT;
  }
  conf=Utils.initializeYarnConfiguration();
  LOG.info("Copy App Master jar from local filesystem and add to local environment");
  final FileSystem fs=FileSystem.get(conf);
  if (!fs.getClass().getSimpleName().equals("GoogleHadoopFileSystem") && fs.getScheme().startsWith("file")) {
    LOG.warn("The file system scheme is '" + fs.getScheme() + "'. This indicates that the "+ "specified Hadoop configuration path is wrong and the sytem is using the default Hadoop configuration values."+ "The Flink YARN client needs to store its files in a distributed file system");
  }
  yarnClient=YarnClient.createYarnClient();
  yarnClient.init(conf);
  yarnClient.start();
  if (cmd.hasOption(QUERY.getOpt())) {
    showClusterMetrics(yarnClient);
  }
  if (!cmd.hasOption(CONTAINER.getOpt())) {
    LOG.error("Missing required argument " + CONTAINER.getOpt());
    printUsage();
    yarnClient.stop();
    System.exit(1);
  }
  final int taskManagerCount=Integer.valueOf(cmd.getOptionValue(CONTAINER.getOpt()));
  System.out.println("Using values:");
  System.out.println("\tContainer Count = " + taskManagerCount);
  System.out.println("\tJar Path = " + localJarPath.toUri().getPath());
  System.out.println("\tConfiguration file = " + confPath.toUri().getPath());
  System.out.println("\tJobManager memory = " + jmMemory);
  System.out.println("\tTaskManager memory = " + tmMemory);
  System.out.println("\tTaskManager cores = " + tmCores);
  YarnClientApplication app=yarnClient.createApplication();
  GetNewApplicationResponse appResponse=app.getNewApplicationResponse();
  Resource maxRes=appResponse.getMaximumResourceCapability();
  if (tmMemory > maxRes.getMemory() || tmCores > maxRes.getVirtualCores()) {
    LOG.error("The cluster does not have the requested resources for the TaskManagers available!\n" + "Maximum Memory: " + maxRes.getMemory() + ", Maximum Cores: "+ tmCores);
    yarnClient.stop();
    System.exit(1);
  }
  if (jmMemory > maxRes.getMemory()) {
    LOG.error("The cluster does not have the requested resources for the JobManager available!\n" + "Maximum Memory: " + maxRes.getMemory());
    yarnClient.stop();
    System.exit(1);
  }
  int totalMemoryRequired=jmMemory + tmMemory * taskManagerCount;
  ClusterResourceDescription freeClusterMem=getCurrentFreeClusterResources(yarnClient);
  if (freeClusterMem.totalFreeMemory < totalMemoryRequired) {
    LOG.error("This YARN session requires " + totalMemoryRequired + "MB of memory in the cluster. "+ "There are currently only "+ freeClusterMem.totalFreeMemory+ "MB available.");
    yarnClient.stop();
    System.exit(1);
  }
  if (tmMemory > freeClusterMem.containerLimit) {
    LOG.error("The requested amount of memory for the TaskManagers (" + tmMemory + "MB) is more than "+ "the largest possible YARN container: "+ freeClusterMem.containerLimit);
    yarnClient.stop();
    System.exit(1);
  }
  if (jmMemory > freeClusterMem.containerLimit) {
    LOG.error("The requested amount of memory for the JobManager (" + jmMemory + "MB) is more than "+ "the largest possible YARN container: "+ freeClusterMem.containerLimit);
    yarnClient.stop();
    System.exit(1);
  }
  final String javaOpts=GlobalConfiguration.getString(ConfigConstants.FLINK_JVM_OPTIONS,"");
  ContainerLaunchContext amContainer=Records.newRecord(ContainerLaunchContext.class);
  String amCommand="$JAVA_HOME/bin/java" + " -Xmx" + Utils.calculateHeapSize(jmMemory) + "M "+ javaOpts;
  if (hasLogback || hasLog4j) {
    amCommand+=" -Dlog.file=\"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/jobmanager-main.log\"";
  }
  if (hasLogback) {
    amCommand+=" -Dlogback.configurationFile=file:logback.xml";
  }
  if (hasLog4j) {
    amCommand+=" -Dlog4j.configuration=file:log4j.properties";
  }
  amCommand+=" " + ApplicationMaster.class.getName() + " "+ " 1>"+ ApplicationConstants.LOG_DIR_EXPANSION_VAR+ "/jobmanager-stdout.log"+ " 2>"+ ApplicationConstants.LOG_DIR_EXPANSION_VAR+ "/jobmanager-stderr.log";
  amContainer.setCommands(Collections.singletonList(amCommand));
  System.err.println("amCommand=" + amCommand);
  ApplicationSubmissionContext appContext=app.getApplicationSubmissionContext();
  final ApplicationId appId=appContext.getApplicationId();
  int appNumber=appId.getId();
  jmPort=Utils.offsetPort(jmPort,appNumber);
  LocalResource appMasterJar=Records.newRecord(LocalResource.class);
  LocalResource flinkConf=Records.newRecord(LocalResource.class);
  Path remotePathJar=Utils.setupLocalResource(conf,fs,appId.toString(),localJarPath,appMasterJar,fs.getHomeDirectory());
  Path remotePathConf=Utils.setupLocalResource(conf,fs,appId.toString(),confPath,flinkConf,fs.getHomeDirectory());
  Map<String,LocalResource> localResources=new HashMap<String,LocalResource>(2);
  localResources.put("flink.jar",appMasterJar);
  localResources.put("flink-conf.yaml",flinkConf);
  final Path[] paths=new Path[3 + shipFiles.size()];
  StringBuffer envShipFileList=new StringBuffer();
  for (int i=0; i < shipFiles.size(); i++) {
    File shipFile=shipFiles.get(i);
    LocalResource shipResources=Records.newRecord(LocalResource.class);
    Path shipLocalPath=new Path("file://" + shipFile.getAbsolutePath());
    paths[3 + i]=Utils.setupLocalResource(conf,fs,appId.toString(),shipLocalPath,shipResources,fs.getHomeDirectory());
    localResources.put(shipFile.getName(),shipResources);
    envShipFileList.append(paths[3 + i]);
    if (i + 1 < shipFiles.size()) {
      envShipFileList.append(',');
    }
  }
  paths[0]=remotePathJar;
  paths[1]=remotePathConf;
  sessionFilesDir=new Path(fs.getHomeDirectory(),".flink/" + appId.toString() + "/");
  FsPermission permission=new FsPermission(FsAction.ALL,FsAction.ALL,FsAction.ALL);
  fs.setPermission(sessionFilesDir,permission);
  Utils.setTokensFor(amContainer,paths,this.conf);
  amContainer.setLocalResources(localResources);
  fs.close();
  Map<String,String> appMasterEnv=new HashMap<String,String>();
  Utils.setupEnv(conf,appMasterEnv);
  appMasterEnv.put(Client.ENV_TM_COUNT,String.valueOf(taskManagerCount));
  appMasterEnv.put(Client.ENV_TM_CORES,String.valueOf(tmCores));
  appMasterEnv.put(Client.ENV_TM_MEMORY,String.valueOf(tmMemory));
  appMasterEnv.put(Client.FLINK_JAR_PATH,remotePathJar.toString());
  appMasterEnv.put(Client.ENV_APP_ID,appId.toString());
  appMasterEnv.put(Client.ENV_CLIENT_HOME_DIR,fs.getHomeDirectory().toString());
  appMasterEnv.put(Client.ENV_CLIENT_SHIP_FILES,envShipFileList.toString());
  appMasterEnv.put(Client.ENV_CLIENT_USERNAME,UserGroupInformation.getCurrentUser().getShortUserName());
  appMasterEnv.put(Client.ENV_SLOTS,String.valueOf(slots));
  appMasterEnv.put(Client.ENV_APP_NUMBER,String.valueOf(appNumber));
  if (dynamicPropertiesEncoded != null) {
    appMasterEnv.put(Client.ENV_DYNAMIC_PROPERTIES,dynamicPropertiesEncoded);
  }
  amContainer.setEnvironment(appMasterEnv);
  Resource capability=Records.newRecord(Resource.class);
  capability.setMemory(jmMemory);
  capability.setVirtualCores(1);
  appContext.setApplicationName("Flink");
  appContext.setAMContainerSpec(amContainer);
  appContext.setResource(capability);
  appContext.setQueue(queue);
  yarnPropertiesFile=new File(confDirPath + CliFrontend.YARN_PROPERTIES_FILE);
  LOG.info("Submitting application master " + appId);
  yarnClient.submitApplication(appContext);
  ApplicationReport appReport=yarnClient.getApplicationReport(appId);
  Runtime.getRuntime().addShutdownHook(new ClientShutdownHook());
  LOG.info("Start actor system.");
  actorSystem=AkkaUtils.createActorSystem();
  String path=appReport.getHost() + ":" + jmPort;
  LOG.info("Start application client.");
  applicationClient=actorSystem.actorOf(Props.create(ApplicationClient.class,path,yarnClient));
  appReport=AkkaUtils.ask(applicationClient,WaitForJobTermination$.MODULE$,AkkaUtils.FUTURE_TIMEOUT(),Duration.Inf());
  LOG.info("Application " + appId + " finished with state "+ appReport.getYarnApplicationState()+ " and final state "+ appReport.getFinalApplicationStatus()+ " at "+ appReport.getFinishTime());
  if (appReport.getYarnApplicationState() == YarnApplicationState.FAILED || appReport.getYarnApplicationState() == YarnApplicationState.KILLED) {
    LOG.warn("Application failed. Diagnostics " + appReport.getDiagnostics());
    LOG.warn("If log aggregation is activated in the Hadoop cluster, we recommend to retreive " + "the full application log using this command:\n" + "\tyarn logs -applicationId " + appReport.getApplicationId() + "\n"+ "(It sometimes takes a few seconds until the logs are aggregated)");
  }
}
